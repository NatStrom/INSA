{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c9a06a-4333-49ff-8c9d-81a5a024126b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! pip install keras-nlp\n",
    "#! pip install tensorflow\n",
    "#! pip install keras\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2868e0e5-5943-4389-8c9e-6d40bdbc57e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/natalies_macbook/Documents/GitHub/INSA/EU/klassifikation\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5b2dd6-32f0-4fbf-ab05-d526dc173063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.'C:\\Users\\cecilianatalie.strom\\OneDrive - GIGA\\Desktop\\INSA_Desktop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c4f138-c53d-4296-8c7d-56c4b22cf1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Umgebungsvariable, keras nutzt tf\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5566b0f-177d-415e-bfed-e7ee876da135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/cecilianatalie.strom/OneDrive - GIGA/Desktop/INSA_Desktop/EU_lex/nlp_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/cecilianatalie.strom/OneDrive - GIGA/Desktop/INSA_Desktop/EU_lex/nlp_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/cecilianatalie.strom/OneDrive - GIGA/Desktop/INSA_Desktop/EU_lex/nlp_test'"
     ]
    }
   ],
   "source": [
    "os.listdir(\"C:/Users/cecilianatalie.strom/OneDrive - GIGA/Desktop/INSA_Desktop/EU_lex/nlp_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cb0ee-1d60-42df-9575-7bfc37596163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create our dataset\n",
    "\n",
    "INSA_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"C:/Users/cecilianatalie.strom/OneDrive - GIGA/Desktop/INSA_Desktop/EU_lex/nlp_test\",\n",
    "    batch_size=4,\n",
    "    seed=42,\n",
    "    labels=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d356c-5ebe-459e-97db-6186393053ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert data to lowercase\n",
    "INSA_ds = INSA_ds.map(lambda x: (tf.strings.lower(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1be19-45fd-4400-97b1-e32480561d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load tokenizer from disk\n",
    "with open(\"C:/Users/cecilianatalie.strom/GitHub/INSA/EU/klassifikation/INSA_vocabulary.pickle\", \"rb\") as handle:\n",
    "    vocabulary = pickle.load(handle)\n",
    "    \n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocabulary,\n",
    "    lowercase=False,\n",
    "    sequence_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098b4ce-a1e6-4b4a-ae77-c5a16612ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize dataset\n",
    "def format_dataset(sentence):\n",
    "    sentence = tokenizer(sentence)\n",
    "    return sentence\n",
    "\n",
    "def make_dataset(dataset):\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "INSA_ds_eval = make_dataset(INSA_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650d743-1b0e-4efd-b84f-9dfcefccde80",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSA_fnet_classifier = keras_nlp.models.Classifier.from_preset(\n",
    "    \"C:/Users/cecilianatalie.strom/GitHub/INSA/EU/klassifikation/INSA_classifier.h5\",\n",
    "    num_classes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32305ec6-8486-4b7d-8986-dd929fd3bbfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict new class\n",
    "INSA_fnet_classifier(dataset[0])\n",
    "#\n",
    "#INSA_fnet_classifier(dataset[0][0])\n",
    "#im loop anwenden, in zwei unterordner verschieben"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
