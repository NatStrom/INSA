{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6cf999-032e-4899-b9e6-517384d2731b",
   "metadata": {},
   "source": [
    "# this script builds a loop over all documents to extract the beccessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205ccf91-02cb-4f6f-858f-c80f6f5901b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load all neccessary packages\n",
    "import pandas\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efd6579-a821-4c46-a61f-0e78a580fe44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the directory of the datafiles I want to loop over\n",
    "user = '/Users/natalies_macbook/Documents/' #change this to your folder where GitHub sits\n",
    "directory = user + 'GitHub/INSA/EU/relevant/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5fb4b75-7e23-45b1-8593-89f74fe3500e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#define all patterns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#search for UN to add the UN dummy indicator\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m UN_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUN\u001b[39m\u001b[38;5;124m'\u001b[39m,data)\n\u001b[1;32m      6\u001b[0m UN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#define all patterns\n",
    "\n",
    "#search for UN to add the UN dummy indicator\n",
    "UN_pattern = re.search(r'UN',data)\n",
    "\n",
    "UN = None\n",
    "for i in data:\n",
    "    if UN_pattern == None:\n",
    "        UN = 0\n",
    "    else:\n",
    "        UN = 1\n",
    "#get the document base Information       \n",
    "url= re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data) #finds the url through the href tag\n",
    "#publication_date = re.findall(r'<p class=\"oj-hd-date\">(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', data)\n",
    "#title = re.findall(r'class=\"oj-doc-ti\"[^>]*>([\\s\\S]*?)<\\/p>', data)\n",
    "title = soup.find('p', class_='oj-doc-ti').get_text(strip=True)\n",
    "celex= re.findall(r'uri=CELEX%([^\"]+)', data) # get the celex id\n",
    "publication_date = soup.find('p', class_='oj-hd-date').get_text(strip=True)\n",
    "\n",
    "#get the document information\n",
    "decision_match = re.findall(r'(COUNCIL.*?)</p', data)\n",
    "legal_decision = re.findall(r'<p[^>]*>(.*?(amending|implementing).*?)</p>',data)\n",
    "legal_base = re.findall(r'<p[^>]*>(.*?regard to.*?)</p>',data)\n",
    "\n",
    "#check for list updates, removals or deletions\n",
    "action_intended = re.findall(r'<p class=\"(oj-normal|oj-bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced)\\s*.*?)\\s*</p>',data)\n",
    "action_intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40bc3d3-ac9b-48c2-83eb-3b4fa82ca1f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m listing_position \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moj-normal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+.*?)\u001b[39m\u001b[38;5;124m'\u001b[39m,data)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(listing_position)\n\u001b[1;32m      3\u001b[0m name_2 \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<p class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moj-normal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*<span class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moj-bold\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>(.*?)</span>\u001b[39m\u001b[38;5;124m'\u001b[39m, data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "listing_position = re.findall(r'class=\"oj-normal\">(\\d+.*?)',data)\n",
    "print(listing_position)\n",
    "name_2 = re.findall(r'<p class=\"oj-normal\">\\s*<span class=\"oj-bold\">(.*?)</span>', data)\n",
    "print(name_2)\n",
    "un_sanction_date = re.findall(r'class=\"oj-normal\">Date of UN designation(.*?)</p>',data,re.DOTALL)\n",
    "print(date_pattern)\n",
    "identifying_information =re.findall(r'>(.*?Date of Birth:.*?)</p>',data)\n",
    "print(identifying_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39eee05c-165c-4d18-8523-b0e3a519ab27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m court_decisions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Iterate over the content\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Get the content of the court case\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     name \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<p class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moj-normal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>(.*?)</p>\u001b[39m\u001b[38;5;124m'\u001b[39m, i)\n\u001b[1;32m     17\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "#court decision matching\n",
    "name_pattern_court = r'Orders (.+?) to pay'\n",
    "#for dismissal entity name\n",
    "# Create patterns\n",
    "delist_pattern = r'Annuls'\n",
    "delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "name_pattern1 = r'Orders (.+?) to pay'\n",
    "\n",
    "# Create a list to store all court decisions\n",
    "court_decisions = []\n",
    "\n",
    "# Iterate over the content\n",
    "for i in content:\n",
    "    # Get the content of the court case\n",
    "    name = re.findall(r'<p class=\"oj-normal\">(.*?)</p>', i)\n",
    "    text = ''.join(name)\n",
    "    \n",
    "    # Initialize variables\n",
    "    Delisting = None\n",
    "    delisting_case = None\n",
    "    entity_name = None\n",
    "    \n",
    "    # Check for delisting\n",
    "    if re.search(delist_pattern, text):\n",
    "        Delisting = 1\n",
    "    else:\n",
    "        Delisting = 0\n",
    "    if Delisting == 1:\n",
    "        match = re.search(name_pattern2, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    if Delisting == 0:\n",
    "        match = re.search(name_pattern1, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    \n",
    "    # Extract delisting case\n",
    "    match = re.search(delisting_case_pattern, text)\n",
    "    if match:\n",
    "        delisting_case = match.group(1)\n",
    "\n",
    "    # Create dictionary for current court decision\n",
    "    court_decision = {\n",
    "        'entity_name': entity_name,\n",
    "        'Delisting': Delisting,\n",
    "        'Delisting_case': delisting_case,\n",
    "        'url': re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data),\n",
    "        'celex' : re.findall(r'uri=CELEX%([^\"]+)', data),\n",
    "        'publication_date': re.findall(r'<p class=\"oj-hd-date\">(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', data),\n",
    "        'date': re.findall(r'<p class=\"oj-doc-ti\">of (\\d+ \\w+ \\d+)</p>', data),\n",
    "        'title': re.findall(r'<p class=\"oj-doc-ti\" id=\"[^\"]*\">(.*?)</p>', data),\n",
    "        'law_number': re.findall(r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>', data),\n",
    "        'case_number': re.findall(r'Case\\sT-\\d+/\\d+', data),\n",
    "        'legal_base': re.findall(r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)', data)\n",
    "    }\n",
    "    \n",
    "    # Append the current decision dictionary to the list\n",
    "    court_decisions.append(court_decision)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(court_decisions, encoding = 'UTF-8') #encoding Ã¤ndern oder string subsitute (string punkt replace xa0 mit .)\n",
    "#now extract th information\n",
    "# Initialize variables\n",
    "Delisting = None\n",
    "delisting_case = None\n",
    "entity_name = None\n",
    "#create patterns\n",
    "delist_pattern = r'The Court: 1. Annuls'\n",
    "delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "name_pattern = r'in so far as they concern (.+?);2.Orders'\n",
    "#create empty dic\n",
    "results = {}\n",
    "\n",
    "for i in content:\n",
    "    #get the content of the court case\n",
    "    name = re.findall(r'<p class=\"oj-normal\">(.*?)</p>', i)\n",
    "    text = ''.join(name)\n",
    "    if re.search(delist_pattern, text):\n",
    "        Delisting = 0\n",
    "        print('Dismissal')\n",
    "    else:\n",
    "        Delisting = 1  # Assuming 1 for non-dismissal cases\n",
    "    match = re.search(delisting_case_pattern, text)\n",
    "    if match:\n",
    "        delisting_case = match.group(1)\n",
    "    # Extract entity name\n",
    "    match = re.search(name_pattern, text)\n",
    "    if match:\n",
    "        entity_name = match.group(1)\n",
    "#now get it into a dictionary format\n",
    "court_decision = {}\n",
    "court_decision['entity_name'] = entity_name\n",
    "court_decision['Delisting'] = Delisting\n",
    "court_decision['Delisting_case']= delisting_case\n",
    "court_decision['url'] = re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data)\n",
    "court_decision['publication_date'] =re.findall(r'<p class=\"oj-hd-date\">(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', data)\n",
    "court_decision['date'] =  re.findall(r'<p class=\"oj-doc-ti\">of (\\d+ \\w+ \\d+)</p>', data)\n",
    "court_decision['title']= re.findall(r'<p class=\"oj-doc-ti\" id=\"[^\"]*\">(.*?)</p>', data)\n",
    "court_decision['law_number'] = re.findall(r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)',data)\n",
    "court_decision['case_number'] = re.findall(r'Case\\sT-\\d+/\\d+', data)\n",
    "court_decision['legal_base'] = re.findall(r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006295be-f01d-43a3-a90d-cb66001f7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define all the patterns\n",
    "url = re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data) #finds the url through the href tag\n",
    "#publication_date = re.findall(r'<p class=\"oj-hd-date\">(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', data)\n",
    "#title = re.findall(r'class=\"oj-doc-ti\"[^>]*>([\\s\\S]*?)<\\/p>', data)\n",
    "title = soup.find('p', class_='oj-doc-ti').get_text(strip=True)\n",
    "celex= re.findall(r'uri=CELEX%([^\"]+)', data) # get the celex id\n",
    "publication_date = soup.find('p', class_='oj-hd-date').get_text(strip=True)\n",
    "\n",
    "#get the document information\n",
    "decision_match = re.findall(r'(COUNCIL.*?)</p', data)\n",
    "legal_decision = re.findall(r'<p[^>]*>(.*?(amending|implementing).*?)</p>',data)\n",
    "legal_base = re.findall(r'<p[^>]*>(.*?regard to.*?)</p>',data)\n",
    "\n",
    "#check for list updates, removals or deletions\n",
    "action_intended = re.findall(r'<p class=\"(oj-normal|oj-bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced)\\s*.*?)\\s*</p>',data)\n",
    "action_intended        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64f28530-f60d-4974-9177-227f46bc67aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write the loop\n",
    "for dirpath, dirnames, filenames in os.walk(directory):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                with open(file_path, encoding='utf-8') as f:\n",
    "                    data = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(file_path, encoding='latin-1') as f:\n",
    "                    data = f.read()\n",
    "                    \n",
    "            soup = BeautifulSoup(data, 'html.parser')\n",
    "            \n",
    "            # Define all the patterns\n",
    "            url = re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data) # finds the url through the href tag\n",
    "            celex = re.findall(r'uri=CELEX%([^\"]+)', data) # get the celex id\n",
    "            title = soup.find('p', class_='oj-doc-ti').get_text(strip=True) if soup.find('p', class_='oj-doc-ti') else ''\n",
    "            publication_date = soup.find('p', class_='oj-hd-date').get_text(strip=True) if soup.find('p', class_='oj-hd-date') else ''\n",
    "            decision_match = re.findall(r'(COUNCIL.*?)</p', data)\n",
    "            legal_decision = re.findall(r'<p[^>]*>(.*?(amending|implementing).*?)</p>', data)\n",
    "            legal_base = re.findall(r'<p[^>]*>(.*?regard to.*?)</p>', data)\n",
    "            action_intended = re.findall(r'<p class=\"(oj-normal|oj-bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced)\\s*.*?)\\s*</p>', data)\n",
    "            \n",
    "            rows = soup.find_all('tr', class_='oj-table')  # Finds all elements tr = tablerow of the class oj-table\n",
    "            for row in rows[1:]:  # Skip the header row\n",
    "                cells = row.find_all('td')  # Finds all td table data in one tablerow\n",
    "                if len(cells) < 5:\n",
    "                    continue\n",
    "                name = cells[1].get_text(strip=True)\n",
    "                identifying_info = cells[2].get_text(strip=False)  # Extracting the identifying information but keeping the newline \n",
    "                reasons = ' '.join(p.get_text(strip=True) for p in cells[3].find_all('p'))\n",
    "                date_of_listing = cells[4].get_text(strip=True)\n",
    "                \n",
    "                # Since url and celex are lists, extract the first element if they exist\n",
    "                url_str = url[0] if url else ''\n",
    "                celex_str = celex[0] if celex else ''\n",
    "                \n",
    "                sanction_dict = {\n",
    "                    'URL': url_str,\n",
    "                    'CELEX': celex_str,\n",
    "                    'law_number': title,\n",
    "                    'Publication Date': publication_date,\n",
    "                    'legal_base': legal_base[0] if legal_base else '',\n",
    "                    'Name': name,\n",
    "                    'Identifying Information': identifying_info,\n",
    "                    'Reason': reasons,\n",
    "                    'Date of Listing': date_of_listing,\n",
    "                    'Decision Match': decision_match[0] if decision_match else '',\n",
    "                    'Legal Decision': legal_decision[0][0] if legal_decision else '',\n",
    "                    'Action Intended': action_intended[0][1] if action_intended else ''\n",
    "                }\n",
    "                sanction_list.append(sanction_dict)\n",
    "\n",
    "# At this point, sanction_list contains the parsed data\n",
    "# You can print or process the sanction_list as needed\n",
    "print(sanction_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2364ba6f-0e11-4beb-a7ab-fd5d1e1a5918",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Parse the HTML content\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Find all rows in the table\u001b[39;00m\n\u001b[1;32m      5\u001b[0m rows \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moj-table\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "# Find all rows in the table\n",
    "rows = soup.find_all('tr', class_='oj-table')\n",
    "\n",
    "# List to store the parsed data\n",
    "sanction_list = []\n",
    "\n",
    "# Iterate through the rows and extract the necessary information\n",
    "for row in rows[1:]:  # Skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) < 5:\n",
    "        continue\n",
    "\n",
    "    name = cells[1].get_text(strip=True)\n",
    "    identifying_info = cells[2].get_text(strip=True)  # Extracting the identifying information\n",
    "    reasons = ' '.join(p.get_text(strip=True) for p in cells[3].find_all('p'))\n",
    "    date_of_listing = cells[4].get_text(strip=True)\n",
    "    \n",
    "    sanction_list.append({\n",
    "        'URL': url,\n",
    "        'CELEX':celex,\n",
    "        'law_number' :decision_match[0],\n",
    "        'Publication Date': publication_date,\n",
    "        'legal_base' : legal_decision[0],\n",
    "        'Name': name,\n",
    "        'Identifying Information': identifying_info,\n",
    "        'Reason': reasons,\n",
    "        'Date of Listing': date_of_listing\n",
    "    })\n",
    "\n",
    "# Display the results\n",
    "for person in sanction_list:\n",
    "    print(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538796f-064c-4877-a0c8-6dd862bb6ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8e809-d621-4169-bbd9-085167ea8974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
