{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6cf999-032e-4899-b9e6-517384d2731b",
   "metadata": {},
   "source": [
    "# this script builds a loop over all documents to extract the beccessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205ccf91-02cb-4f6f-858f-c80f6f5901b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load all neccessary packages\n",
    "import pandas\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efd6579-a821-4c46-a61f-0e78a580fe44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the directory of the datafiles I want to loop over\n",
    "user = '/Users/natalies_macbook/Documents/' #change this to your folder where GitHub sits\n",
    "directory = user + 'GitHub/INSA/EU/relevant'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bad5a-22af-41e7-b186-b61b37af3953",
   "metadata": {
    "tags": []
   },
   "source": [
    "# patterns for court decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8966596-63f6-497c-a6b4-e4d92d2b20b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "court_decision_pattern = r'<p class=\"oj-doc-ti\".*?>Judgment of the General Court.*?</p>'\n",
    "\n",
    "def extract_court_decision(html_content):\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Define the patterns\n",
    "    delist_pattern = r'Annuls'\n",
    "    delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "    name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "    name_pattern1 = r'Orders (.+?) to pay'\n",
    "    \n",
    "    # Initialize variables\n",
    "    Delisting = 0  # Default to 0 (not a delisting)\n",
    "    delisting_case = None\n",
    "    entity_name = None\n",
    "    \n",
    "    # Extract the main text content from the relevant <p> tags\n",
    "    paragraphs = soup.find_all('p', class_='oj-normal')\n",
    "    text = ' '.join(p.get_text() for p in paragraphs)\n",
    "    \n",
    "    # Check for delisting\n",
    "    if re.search(delist_pattern, text):\n",
    "        Delisting = 1  # Set to 1 if it matches the delisting pattern\n",
    "        match = re.search(name_pattern2, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    else:\n",
    "        match = re.search(name_pattern1, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    \n",
    "    # Extract delisting case\n",
    "    match = re.search(delisting_case_pattern, text)\n",
    "    if match:\n",
    "        delisting_case = match.group(1)\n",
    "    \n",
    "    # Extract additional details using BeautifulSoup and regex\n",
    "    return {\n",
    "        'entity_name': entity_name,\n",
    "        'Delisting': Delisting,  # Binary indicator (1 for delisting, 0 otherwise)\n",
    "        'Delisting_case': delisting_case,\n",
    "        'url': [a['href'] for a in soup.find_all('a', href=True) if '.eu/legal-content/EN/TXT/HTML/' in a['href']],\n",
    "        'celex': re.findall(r'uri=CELEX%([^\"]+)', str(soup)),\n",
    "        'publication_date': re.findall(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', str(soup.find('p', class_='oj-hd-date'))),\n",
    "        'date': re.findall(r'of (\\d+ \\w+ \\d+)', str(soup.find('p', class_='oj-doc-ti'))),\n",
    "        'title': [t.get_text() for t in soup.find_all('p', class_='oj-doc-ti')],\n",
    "        'law_number': re.findall(r'C (\\d+/\\d+)', str(soup.find('p', class_='oj-hd-oj'))),\n",
    "        'case_number': re.findall(r'Case\\sT-\\d+/\\d+', str(soup)),\n",
    "        'legal_base': re.findall(r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)', str(soup))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ed5a9-1eb7-4473-8722-ae6833d1e0a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# patterns for notices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b533fead-0ac8-4fa2-a34a-351810824a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "notice_pattern = r'<p class=\"oj-doc-ti\".*?\">Notice.*?</p>'\n",
    "\n",
    "def notice_pattern_extraction(soup):\n",
    "    # Document base information\n",
    "    url_pattern = r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"'\n",
    "    celex_pattern = r'uri=CELEX%([^\"]+)'\n",
    "    \n",
    "    # Extract publication date and title\n",
    "    publication_date = soup.find('p', class_='oj-hd-date')\n",
    "    publication_date = publication_date.get_text(strip=True) if publication_date else None\n",
    "    \n",
    "    title = soup.find('p', class_='oj-doc-ti')\n",
    "    title = title.get_text(strip=True) if title else None\n",
    "    # turn soup into string\n",
    "    soup_str = str(soup)\n",
    "    # Extract URL and CELEX\n",
    "    url_match = re.search(url_pattern, soup_str)\n",
    "    celex_match = re.search(celex_pattern, soup_str)\n",
    "    url = url_match.group(1) if url_match else None\n",
    "    celex = celex_match.group(1) if celex_match else None\n",
    "    \n",
    "    # Document information patterns\n",
    "    decision_match_pattern = r'(Decision[^,]*|Regulation[^,]*|Annex[^,]*)'\n",
    "    legal_action_pattern = r'<p[^>]*>(.*?(implementing|amended|implemented).*?)</p>'\n",
    "    legal_base_pattern = r'<p[^>]*>(.*?regard to|amending.*?)</p>'\n",
    "    case_number_pattern = r'Case\\sT-\\d+/\\d+'\n",
    "    law_number_pattern = r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>'\n",
    "    legal_base_alt_pattern = r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)'\n",
    "    \n",
    "    decision_match = re.search(decision_match_pattern, soup_str)\n",
    "    legal_action = re.search(legal_action_pattern, soup_str)\n",
    "    legal_base = re.search(legal_base_pattern, soup_str)\n",
    "    case_number = re.search(case_number_pattern, soup_str)\n",
    "    law_number = re.search(law_number_pattern, soup_str)\n",
    "    legal_base_alt = re.search(legal_base_alt_pattern, soup_str)\n",
    "    \n",
    "    # List updates, removals, deletions\n",
    "    action_intended_pattern = r'<p class=\"(oj-normal|oj-bold|normal|bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced|amended)\\s*.*?)\\s*</p>'\n",
    "    action_intended = re.search(action_intended_pattern, str(soup))\n",
    "    notice_target = re.search(r'The following information is brought to the attention of\\s*(.*?),', soup_str)\n",
    "    \n",
    "    return {'publication_date': publication_date,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'celex': celex,\n",
    "            'decision_match': decision_match.group(1) if decision_match else None,\n",
    "            'legal_action': legal_action.group(1) if legal_action else None,\n",
    "            'legal_base': legal_base.group(1) if legal_base else None,\n",
    "            'case_number': case_number.group(0) if case_number else None,\n",
    "            'law_number': law_number.group(1) if law_number else None,\n",
    "            'legal_base_alt': legal_base_alt.group(0) if legal_base_alt else None,\n",
    "            'action_intended': action_intended.group(2) if action_intended else None,\n",
    "            'notice_target':notice_target.group(1) if notice_target else None}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8fa8e-99e9-4144-9b26-76398bb1e871",
   "metadata": {},
   "source": [
    "# main pattern all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1a5265-51a8-4e55-91d4-e51cc78565f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully memorized the listing patterns\n"
     ]
    }
   ],
   "source": [
    "#paterns for the general listing documents\n",
    "def extract_document_info(soup):\n",
    "    # Document base information\n",
    "    url_pattern = r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"'\n",
    "    celex_pattern = r'uri=CELEX%([^\"]+)'\n",
    "    \n",
    "    # Extract publication date and title\n",
    "    publication_date = soup.find('p', class_='oj-hd-date')\n",
    "    publication_date = publication_date.get_text(strip=True) if publication_date else None\n",
    "    \n",
    "    title = soup.find('p', class_='oj-doc-ti')\n",
    "    title = title.get_text(strip=True) if title else None\n",
    "    # turn soup into string\n",
    "    soup_str = str(soup)\n",
    "    # Extract URL and CELEX\n",
    "    url_match = re.search(url_pattern, soup_str)\n",
    "    celex_match = re.search(celex_pattern, soup_str)\n",
    "    url = url_match.group(1) if url_match else None\n",
    "    celex = celex_match.group(1) if celex_match else None\n",
    "    \n",
    "    # Document information patterns\n",
    "    decision_match_pattern = r'(Decision[^,]*|Regulation[^,]*|Annex[^,]*)</p>'\n",
    "    legal_action_pattern = r'<p[^>]*>.*?(implementing|amended|implemented).*?</p>'\n",
    "    legal_base_pattern = r'<p[^>]*>(.*?regard to|amending.*?)</p>'\n",
    "    case_number_pattern = r'Case\\sT-\\d+/\\d+'\n",
    "    law_number_pattern = r'<p class=\"oj-hd-oj\">(C (\\d+/\\d+))</p>'\n",
    "    legal_base_alt_pattern = r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)'\n",
    "    \n",
    "    decision_match = re.search(decision_match_pattern, soup_str)\n",
    "    legal_action = re.search(legal_action_pattern, soup_str)\n",
    "    legal_base = re.search(legal_base_pattern, soup_str)\n",
    "    case_number = re.search(case_number_pattern, soup_str)\n",
    "    law_number = re.search(law_number_pattern, soup_str)\n",
    "    legal_base_alt = re.search(legal_base_alt_pattern, soup_str)\n",
    "    \n",
    "    # List updates, removals, deletions\n",
    "    action_intended_pattern = r'<p class=\"(oj-normal|oj-bold|normal|bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced|amended)\\s*.*?)\\s*</p>'\n",
    "    action_intended = re.search(action_intended_pattern, str(soup))\n",
    "    \n",
    "    # Annex with numbers\n",
    "    listing_position_pattern = r'class=\"oj-normal|normal\">(\\d+.*?)'\n",
    "    name_2_pattern = r'<p class=\"oj-normal|normal\">\\s*<span class=\"oj-bold|bold\">(.*?)</span>'\n",
    "    un_sanction_date_pattern = r'class=\"oj-normal|normal\">Date of UN designation(.*?)</p>'\n",
    "    identifying_information_pattern = r'>(.*?Date of Birth:|DOB.*?)</p>'\n",
    "    \n",
    "    listing_position = re.search(listing_position_pattern, soup_str)\n",
    "    name_2 = re.search(name_2_pattern, soup_str)\n",
    "    un_sanction_date = re.search(un_sanction_date_pattern, soup_str)\n",
    "    un_sanction = '1' if un_sanction_date else '0'\n",
    "    identifying_information = re.search(identifying_information_pattern, soup_str) #also fix this here!\n",
    "    \n",
    "    # Return all extracted information\n",
    "    return {\n",
    "        'publication_date': publication_date,\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'celex': celex,\n",
    "        'decision_match': decision_match.group(1) if decision_match else None,\n",
    "        'legal_action': legal_action.group(1) if legal_action else None,\n",
    "        'legal_base': legal_base.group(1) if legal_base else None,\n",
    "        'case_number': case_number.group(0) if case_number else None,\n",
    "        'law_number': law_number.group(1) if law_number else None,\n",
    "        'legal_base_alt': legal_base_alt.group(0) if legal_base_alt else None,\n",
    "        'action_intended': action_intended.group(2) if action_intended else None,\n",
    "        'listing_position': listing_position.group(1) if listing_position else None,\n",
    "        'name_2': name_2.group(1) if name_2 else None,\n",
    "        'un_sanction_date': un_sanction_date.group(1) if un_sanction_date else None,\n",
    "        'un_sanction': un_sanction,\n",
    "        'identifying_information': identifying_information.group(1) if identifying_information else None,\n",
    "    }\n",
    "print('successfully memorized the listing patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e81d8-fdc0-4627-ba34-ae92af1055e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "notice_list = []\n",
    "sanction_list = []\n",
    "court_decision_list = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "        \n",
    "        # Parse the file content with BeautifulSoup\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        \n",
    "        # Extract document-level information\n",
    "        document_info = extract_document_info(soup)\n",
    "        \n",
    "        # Check if the document matches a court decision pattern\n",
    "        if re.findall(court_decision_pattern, str(soup)):\n",
    "            court_info = extract_court_decision(soup)\n",
    "            court_decision_list.append(court_info)\n",
    "        \n",
    "        # Check if the document matches a notice pattern\n",
    "        if re.findall(notice_pattern, str(soup)):\n",
    "            notice_info = notice_pattern_extraction(soup)\n",
    "            notice_list.append(notice_info)\n",
    "        \n",
    "        # Find all table rows of the class 'oj-table' and extract relevant information\n",
    "        rows = soup.find_all('tr', class_='oj-table')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            \n",
    "            # Extract name, alias, and other relevant details\n",
    "            \n",
    "            if cols:\n",
    "                for col in cols:\n",
    "                    header_text = col.get('data-header')  # Replace 'data-header' with the actual attribute or use another method\n",
    "            \n",
    "            if header_text == 'Name':\n",
    "                entity_info['name'] = col.get_text(strip=True)\n",
    "            elif header_text == 'Reasons':\n",
    "                entity_info['reason'] = col.get_text(strip=True)\n",
    "            elif header_text == 'Identifying Information':\n",
    "                entity_info['ident_info'] = col.get_text(strip=True)\n",
    "            elif header_text == 'Date of Listing':\n",
    "                listing_date[''] = col.get_text(strip=True)\n",
    "            # Extract information using regular expressions\n",
    "            position = re.search(r'Position\\(s\\):(.*?)</p>', str(row))\n",
    "            dob = re.search(r'(?:DOB|Date of Birth|Born on): (\\d{1,2}/\\d{1,2}/\\d{4})', str(row))\n",
    "            pob = re.search(r'POB|Place of Birth:(.*?)</p>', str(row))\n",
    "            nationality = re.search(r'Nationality:(.*?)</p>', str(row))\n",
    "            gender = re.search(r'Gender:(.*?)</p>', str(row))\n",
    "            passport = re.search(r'Passport Number:(.*?)</p>', str(row))\n",
    "            social_media = re.search(r'Social media:(.*?)</p>', str(row))\n",
    "            email = re.search(r'Email:(.*?)</p>', str(row))\n",
    "            telephone = re.search(r'Telephone:(.*?)</p>', str(row))\n",
    "            address = re.search(r'Address:(.*?)</p>', str(row))\n",
    "            code = re.search(r'Code:(.*?)</p>', str(row))\n",
    "            date_of_listing = re.search(r'\\d{1,2}/\\d{1,2}/\\d{4}', str(row))\n",
    "            \n",
    "            # Combine document_info with row-specific information into a dictionary\n",
    "            sanction_dict = {\n",
    "                **document_info,\n",
    "                'name_target': name,\n",
    "                'alias': alias,\n",
    "                'position_target': position.group(1) if position else None,\n",
    "                'date_of_birth': dob.group(1) if dob else None,\n",
    "                'place_of_birth': pob.group(1) if pob else None,\n",
    "                'nationality': nationality.group(1) if nationality else None,\n",
    "                'gender': gender.group(1) if gender else None,\n",
    "                'passport_number': passport.group(1) if passport else None,\n",
    "                'social_media': social_media.group(1) if social_media else None,\n",
    "                'email': email.group(1) if email else None,\n",
    "                'telephone': telephone.group(1) if telephone else None,\n",
    "                'address': address.group(1) if address else None,\n",
    "                'code': code.group(1) if code else None,\n",
    "                'date_of_listing': date_of_listing.group(0) if date_of_listing else None\n",
    "            }\n",
    "            \n",
    "            sanction_list.append(sanction_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b0e9821-ad68-4fbe-829e-a1d448efb942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m         date_of_listing \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, ident_info)\u001b[38;5;28;01mif\u001b[39;00m ident_info \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[0;32m---> 69\u001b[0m     row_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row)  \u001b[38;5;66;03m# Convert the entire row to a string for regex matching\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     listing_position_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moj-normal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+.*?)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     71\u001b[0m     name_2_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<p class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moj-normal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*<span class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moj-bold\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>(.*?)</span>\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/bs4/element.py:1659\u001b[0m, in \u001b[0;36mTag.__unicode__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__unicode__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders this PageElement as a Unicode string.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/bs4/element.py:1713\u001b[0m, in \u001b[0;36mTag.decode\u001b[0;34m(self, indent_level, eventual_encoding, formatter, iterator)\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event, element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_stream(iterator):\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m (Tag\u001b[38;5;241m.\u001b[39mSTART_ELEMENT_EVENT, Tag\u001b[38;5;241m.\u001b[39mEMPTY_ELEMENT_EVENT):\n\u001b[0;32m-> 1713\u001b[0m         piece \u001b[38;5;241m=\u001b[39m element\u001b[38;5;241m.\u001b[39m_format_tag(\n\u001b[1;32m   1714\u001b[0m             eventual_encoding, formatter, opening\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m         )\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m Tag\u001b[38;5;241m.\u001b[39mEND_ELEMENT_EVENT:\n\u001b[1;32m   1717\u001b[0m         piece \u001b[38;5;241m=\u001b[39m element\u001b[38;5;241m.\u001b[39m_format_tag(\n\u001b[1;32m   1718\u001b[0m             eventual_encoding, formatter, opening\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/bs4/element.py:1879\u001b[0m, in \u001b[0;36mTag._format_tag\u001b[0;34m(self, eventual_encoding, formatter, opening)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1874\u001b[0m             \u001b[38;5;28misinstance\u001b[39m(val, AttributeValueWithCharsetSubstitution)\n\u001b[1;32m   1875\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m eventual_encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1876\u001b[0m     ):\n\u001b[1;32m   1877\u001b[0m         val \u001b[38;5;241m=\u001b[39m val\u001b[38;5;241m.\u001b[39mencode(eventual_encoding)\n\u001b[0;32m-> 1879\u001b[0m     text \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mattribute_value(val)\n\u001b[1;32m   1880\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1881\u001b[0m         \u001b[38;5;28mstr\u001b[39m(key) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1882\u001b[0m         \u001b[38;5;241m+\u001b[39m formatter\u001b[38;5;241m.\u001b[39mquoted_attribute_value(text))\n\u001b[1;32m   1883\u001b[0m attrs\u001b[38;5;241m.\u001b[39mappend(decoded)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/bs4/formatter.py:128\u001b[0m, in \u001b[0;36mFormatter.attribute_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattribute_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Process the value of an attribute.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    :param ns: A string.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    :return: A string with certain characters replaced by named\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m       or numeric entities.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubstitute(value)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/bs4/formatter.py:119\u001b[0m, in \u001b[0;36mFormatter.substitute\u001b[0;34m(self, ns)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ns\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Substitute.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity_substitution(ns)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/bs4/dammit.py:256\u001b[0m, in \u001b[0;36mEntitySubstitution.substitute_xml\u001b[0;34m(cls, value, make_quoted_attribute)\u001b[0m\n\u001b[1;32m    253\u001b[0m             quote_with \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quote_with \u001b[38;5;241m+\u001b[39m value \u001b[38;5;241m+\u001b[39m quote_with\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubstitute_xml\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value, make_quoted_attribute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Substitute XML entities for special XML characters.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    :param value: A string to be substituted. The less-than sign\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m     quoted, as befits an attribute value.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# Escape angle brackets and ampersands.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#define all the objects\n",
    "notice_list = []\n",
    "sanction_list = []\n",
    "court_decision_list = []\n",
    "ident_info = None\n",
    "reason = None\n",
    "listing_date = None\n",
    "target_name = None\n",
    "\n",
    "# Loop through all files in the directory\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "        \n",
    "        # Parse the file content with BeautifulSoup\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        \n",
    "        # Extract document-level information\n",
    "        document_info = extract_document_info(soup)\n",
    "        \n",
    "        # Check if the document matches a court decision pattern\n",
    "        if re.findall(court_decision_pattern, str(soup)):\n",
    "            court_info = extract_court_decision(str(soup))\n",
    "            court_decision_list.append(court_info)\n",
    "        \n",
    "        # Check if the document matches a notice pattern\n",
    "        if re.findall(notice_pattern, str(soup)):\n",
    "            notice_info = notice_pattern_extraction(soup)\n",
    "            notice_list.append(notice_info)\n",
    "        \n",
    "        # Find all table rows of the class 'oj-table' and extract relevant information\n",
    "        rows = soup.find_all('tr', class_='oj-table')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                for col in cols:\n",
    "                    header_text = col.get('data-header')  # this is for listings in the annex in table format\n",
    "                    \n",
    "                    if header_text == 'Name':\n",
    "                        target_name = col.get_text(strip=True)\n",
    "                    elif header_text == 'Reasons':\n",
    "                        reason = col.get_text(strip=True)\n",
    "                    elif header_text == 'Identifying Information':\n",
    "                        ident_info = col.get_text(strip=True)\n",
    "                    elif header_text == 'Date of Listing':\n",
    "                        listing_date = col.get_text(strip=True)\n",
    "\n",
    "            # Extract additional information using regular expressions\n",
    "                    position = re.search(r'Position\\(s\\):(.*?)</p>', ident_info) if ident_info else None\n",
    "                    dob = re.search(r'(?:DOB|Date of Birth|Born on): (\\d{1,2}/\\d{1,2}/\\d{4})', ident_info) if ident_info else None\n",
    "                    pob = re.search(r'POB|Place of Birth:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    nationality = re.search(r'Nationality:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    gender = re.search(r'Gender:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    passport = re.search(r'Passport Number:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    social_media = re.search(r'Social media:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    email = re.search(r'Email:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    telephone = re.search(r'Telephone:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    address = re.search(r'Address:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    code = re.search(r'Code:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    date_of_listing = re.search(r'\\d{1,2}/\\d{1,2}/\\d{4}', ident_info)if ident_info else None\n",
    "            for row in rows:\n",
    "                row_str = str(row)  # Convert the entire row to a string for regex matching\n",
    "                listing_position_pattern = r'class=\"oj-normal\">(\\d+.*?)'\n",
    "                name_2_pattern = r'<p class=\"oj-normal\">\\s*<span class=\"oj-bold\">(.*?)</span>'\n",
    "                identifying_info_pattern = r'<p class=\"oj-normal\">(.*?)</p>'\n",
    "                un_sanction_date_pattern = r'class=\"oj-normal|normal\">Date of UN designation(.*?)</p>'\n",
    "                listing_position = re.search(listing_position_pattern, row_str)\n",
    "                name_2 = re.search(name_2_pattern, row_str)\n",
    "                un_sanction_date = re.search(un_sanction_date_pattern, row_str)\n",
    "                un_sanction = '1' if un_sanction_date else '0'\n",
    "                identifying_info = re.findall(identifying_info_pattern, row_str)  # Use findall to get multiple <p> tags\n",
    "                if identifying_info:\n",
    "                    name_3 = identifying_info[0]  # Assuming the first <p> contains the name\n",
    "                    if len(identifying_info) > 2:\n",
    "                        reason_3 = identifying_info[2]\n",
    "                    if len(identifying_info) > 1:\n",
    "                        reason_3 = identifying_info[1]\n",
    "                    else:\n",
    "                        reson_3 = None\n",
    "                    dob_3 = next((info for info in identifying_info if \"Date of Birth:\" in info or \"DOB\" in info), None)\n",
    "                    passport_number_3 = next((info for info in identifying_info if \"Passport Number:\" in info), None)\n",
    "                    pob_3 = next((info for info in identifying_info if \"Place of Birth:\" in info or \"POB\" in info), None)\n",
    "\n",
    "            # Combine document_info with row-specific information into a dictionary\n",
    "            sanction_dict = {\n",
    "                **document_info,  # Include document-level information\n",
    "                'name_target': target_name if target_name else None,\n",
    "                #'alias': alias if alias else None,\n",
    "                'reason_txt':reason if reason else None,\n",
    "                'position_target': position if position else None,\n",
    "                'date_of_birth': dob if dob else None,\n",
    "                'place_of_birth': pob if pob else None,\n",
    "                'nationality': nationality if nationality else None,\n",
    "                'gender': gender if gender else None,\n",
    "                'passport_number': passport if passport else None,\n",
    "                'social_media': social_media if social_media else None,\n",
    "                'email': email if email else None,\n",
    "                'telephone': telephone if telephone else None,\n",
    "                'address': address if address else None,\n",
    "                'code': code if code else None,\n",
    "                'date_of_listing': listing_date if listing_date else None,\n",
    "                'listing_position': listing_position.group(1) if listing_position else None,\n",
    "                'name_target_2': name_2.group(1) if name_2 else None,\n",
    "                'un_sanction': un_sanction,\n",
    "                'name_target_3': name_3,\n",
    "                'dob_3': dob_3,\n",
    "                'passport_number_3': passport_number_3,\n",
    "                'pob_3': pob_3\n",
    "            }\n",
    "            sanction_list.append(sanction_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600a8f2-77be-4635-80c7-a775260892b7",
   "metadata": {},
   "source": [
    "*To Do:\n",
    "\n",
    "1. remove html markers like /xa\n",
    "\n",
    "2. clean up identifying_info key\n",
    "\n",
    "3. get rid of listed commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a6242-0516-4aa9-bd2b-1e1aea10e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find_all('tr', class_='oj-table')\n",
    "for row in rows:\n",
    "    row_str = str(row)  # Convert the entire row to a string for regex matching\n",
    "\n",
    "    listing_position_pattern = r'class=\"oj-normal\">(\\d+.*?)'\n",
    "    name_2_pattern = r'<p class=\"oj-normal\">\\s*<span class=\"oj-bold\">(.*?)</span>'\n",
    "    identifying_info_pattern = r'<p class=\"oj-normal\">(.*?)</p>'\n",
    "    un_sanction_date_pattern = r'class=\"oj-normal|normal\">Date of UN designation(.*?)</p>'\n",
    "\n",
    "    listing_position = re.search(listing_position_pattern, row_str)\n",
    "    name_2 = re.search(name_2_pattern, row_str)\n",
    "    un_sanction_date = re.search(un_sanction_date_pattern, row_str)\n",
    "    un_sanction = '1' if un_sanction_date else '0'\n",
    "    identifying_info = re.findall(identifying_info_pattern, row_str)  # Use findall to get multiple <p> tags\n",
    "\n",
    "    if identifying_info:\n",
    "        name_3 = identifying_info[0]  # Assuming the first <p> contains the name\n",
    "        reason_3 = identifying_info[2]\n",
    "\n",
    "        # Look for specific information within the identifying info\n",
    "        dob_3 = next((info for info in identifying_info if \"Date of Birth:\" in info or \"DOB\" in info), None)\n",
    "        passport_number_3 = next((info for info in identifying_info if \"Passport Number:\" in info), None)\n",
    "        pob_3 = next((info for info in identifying_info if \"Place of Birth:\" in info or \"POB\" in info), None)\n",
    "\n",
    "        # Clean up the extracted data\n",
    "        dob_3 = dob_3.strip() if dob_2 else None\n",
    "        passport_number_3 = passport_number_3.strip() if passport_number_3 else None\n",
    "        pob_3 = pob_3.strip() if pob_3 else None\n",
    "\n",
    "        # Example output dictionary, replace with your storage logic\n",
    "        extracted_data = {\n",
    "            'listing_position': listing_position.group(1) if listing_position else None,\n",
    "            'name_2': name_2.group(1) if name_2 else None,\n",
    "            'un_sanction': un_sanction,\n",
    "            'name_3': name_3,\n",
    "            'dob_2': dob_2,\n",
    "            'passport_number_2': passport_number_2,\n",
    "            'pob_2': pob_2\n",
    "        }\n",
    "        \n",
    "        print(extracted_data)  # or store it in a list or a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a2280-c700-450c-a6cc-6d51ae309908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#annex with numbers:\n",
    " rows = soup.find_all('tr', class_='oj-table')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                listing_position_pattern = r'class=\"oj-normal\">(\\d+.*?)'\n",
    "                name_2_pattern = r'<p class=\"oj-normal\">\\s*<span class=\"oj-bold\">(.*?)</span>'\n",
    "                identifying_info_pattern = r'<p class=\"oj-normal\">(.*?)<p/>'\n",
    "                un_sanction_date_pattern = r'class=\"oj-normal|normal\">Date of UN designation(.*?)</p>'\n",
    "                \n",
    "                listing_position = re.search(listing_position_pattern, soup_str)\n",
    "                name_2 = re.search(name_2_pattern, soup_str)\n",
    "                un_sanction_date = re.search(un_sanction_date_pattern, soup_str)\n",
    "                un_sanction = '1' if un_sanction_date else '0'\n",
    "                identifying_info = re.search(identifying_info_pattern, soup_str) #also fix this here!\n",
    "                \n",
    "                name_3 = identifying_info.group(1)\n",
    "                reason = identifying_info.group(3)\n",
    "                dob_2 = re.search('>(Date of Birth:|DOB.*?)</p>',identifying_info)\n",
    "                passport_number_2 = re.search('>(Passport Number:.*?)</p>',identifying_info)\n",
    "                pob_2 = re.search('>(.*?Place of Birth:|POB.*?)</p>',identifying_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f11882-ee17-4c55-b551-036b45622080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00bc28-0ee4-4d7e-a993-79754dd3d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "48-1\">‘ANNEX IV</p>\n",
    "                     <p class=\"oj-doc-ti oj-quotation-ti\">List of natural or legal persons, entities or bodies, referred to in Article 2(7), 2a(7) and 2b(1)</p>\n",
    "    \n",
    "     <table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                        <col width=\"100%\"/>\n",
    "                        <tbody>\n",
    "                           <tr>\n",
    "                              <td valign=\"top\"  >\n",
    "                                 <p class=\"oj-normal\">JSC Sirius</p>\n",
    "                              </td>\n",
    "                           </tr>\n",
    "                        </tbody>\n",
    "                     </table>\n",
    "                     <table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                        <col width=\"100%\"/>\n",
    "           \n",
    "        \n",
    "        <p class=\"oj-doc-ti\" id=\"d1e34-59-1\">ANNEX I</p>\n",
    "            <div>\n",
    "                <div>\n",
    "                    <div>\n",
    "                        <p class=\"oj-doc-ti oj-quotation-ti\" id=\"d1e41-59-1\">\n",
    "                                             ‘ANNEX I</p>\n",
    "                        <p id=\"d1e48-59-1\" class=\"oj-ti-grseq-1\">\n",
    "                            <span class=\"oj-bold\">List of persons referred to in Article 5(1)(a)</span>\n",
    "                        </p>\n",
    "                        <table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                            <col width=\"4%\"/>\n",
    "                            <col width=\"96%\"/>\n",
    "                            <tbody>\n",
    "                                <tr>\n",
    "                                    <td valign=\"top\">\n",
    "                                        <p class=\"oj-normal\">1.</p>\n",
    "                                    </td>\n",
    "                                    <td valign=\"top\">\n",
    "                                        <p class=\"oj-normal\">\n",
    "                                            <span class=\"oj-bold\">AL-BAGHDADI, Dr Abdulqader Mohammed</span>\n",
    "                                        </p>\n",
    "                                        <p class=\"oj-normal\">Passport number: B010574. Date of birth: 1.7.1950.</p>\n",
    "                                        <p class=\"oj-normal\">Head of the Liaison Office of the Revolutionary Committees. Revolutionary Committees involved in violence against demonstrators.</p>\n",
    "                                        <p class=\"oj-normal\">Date of UN designation: 26.2.2011.</p>\n",
    "                                    </td>\n",
    "                                </tr>\n",
    "                            </tbody>\n",
    "                        </table>\n",
    "                        <table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n",
    "                            <col width=\"4%\"/>\n",
    "                            <col width=\"96%\"/>\n",
    "                            <tbody>\n",
    "                                <tr>\n",
    "                                    <td valign=\"top\">\n",
    "                                        <p class=\"oj-normal\">2.</p>\n",
    "                                    </td>\n",
    "                                    <td valign=\"top\">\n",
    "                                        <p class=\"oj-normal\">\n",
    "                                            <span class=\"oj-bold\">DIBRI, Abdulqader Yusef</span>\n",
    "                                        </p>\n",
    "                                        <p class=\"oj-normal\">Date <tbody>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa83ab-5315-4b72-b8db-56ad119e25e0",
   "metadata": {},
   "source": [
    "# pickling the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1f429-ae81-4618-b201-0e0af46a3b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"batched_sanction_EU\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(sanction_list, fp)\n",
    "\n",
    "with open(\"batched_notice_EU\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(notice_list, fp)\n",
    "\n",
    "with open(\"batched_court_decision_EU\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(court_decision_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef7bf9-0535-4cb6-9d48-a0b855a85fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with open(\"batched_sanction_EU\", \"rb\") as fp:   #Pickling\n",
    "#    pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755684fc-106f-48ef-b3a0-a8d8633e39f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define all the objects\n",
    "notice_list = []\n",
    "sanction_list = []\n",
    "court_decision_list = []\n",
    "ident_info = None\n",
    "reason = None\n",
    "listing_date = None\n",
    "target_name = None\n",
    "\n",
    "# Function to extract entity type based on preceding text\n",
    "def extract_entity_type(element):\n",
    "    person_pattern = re.compile(r'List of persons referred to', re.IGNORECASE)\n",
    "    entity_pattern = re.compile(r'List of entities referred to', re.IGNORECASE)\n",
    "\n",
    "    text = element.get_text(strip=True)\n",
    "    if person_pattern.search(text):\n",
    "        return 'person'\n",
    "    elif entity_pattern.search(text):\n",
    "        return 'entity'\n",
    "    return None\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "\n",
    "        # Parse the file content with BeautifulSoup\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "        # Extract document-level information\n",
    "        document_info = extract_document_info(soup)\n",
    "\n",
    "        # Check if the document matches a court decision pattern\n",
    "        if re.findall(court_decision_pattern, str(soup)):\n",
    "            court_info = extract_court_decision(soup)\n",
    "            court_decision_list.append(court_info)\n",
    "\n",
    "        # Check if the document matches a notice pattern\n",
    "        if re.findall(notice_pattern, str(soup)):\n",
    "            notice_info = notice_pattern_extraction(soup)\n",
    "            notice_list.append(notice_info)\n",
    "\n",
    "        # Initialize the current entity type variable\n",
    "        current_entity_type = None\n",
    "\n",
    "        # Iterate through all elements in the document\n",
    "        for element in soup.find_all(['p', 'table', 'ul', 'ol']):\n",
    "            # Update the entity type based on preceding text\n",
    "            entity_type = extract_entity_type(element)\n",
    "            if entity_type:\n",
    "                current_entity_type = entity_type\n",
    "\n",
    "            # Process elements based on the current entity type\n",
    "            if current_entity_type:\n",
    "                # Process tables\n",
    "                if element.name == 'table':\n",
    "                    rows = element.find_all('tr', class_='oj-table')\n",
    "                    for row in rows:\n",
    "                        cols = row.find_all('td')\n",
    "                        if cols:\n",
    "                            sanction_dict = {}\n",
    "                            for col in cols:\n",
    "                                header_text = col.get('data-header')\n",
    "                                \n",
    "                                if header_text == 'Name':\n",
    "                                    target_name = col.get_text(strip=True)\n",
    "                                elif header_text == 'Reasons':\n",
    "                                    reason = col.get_text(strip=True)\n",
    "                                elif header_text == 'Identifying Information':\n",
    "                                    ident_info = col.get_text(strip=True)\n",
    "                                elif header_text == 'Date of Listing':\n",
    "                                    listing_date = col.get_text(strip=True)\n",
    "\n",
    "                                # Extract additional information using regular expressions\n",
    "                                position = re.search(r'Position\\(s\\):(.*?)</p>', str(col)) if ident_info else None\n",
    "                                dob = re.search(r'(?:DOB|Date of Birth|Born on): (\\d{1,2}/\\d{1,2}/\\d{4})', str(col)) if ident_info else None\n",
    "                                pob = re.search(r'POB|Place of Birth:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                nationality = re.search(r'Nationality:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                gender = re.search(r'Gender:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                passport = re.search(r'Passport Number:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                social_media = re.search(r'Social media:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                email = re.search(r'Email:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                telephone = re.search(r'Telephone:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                address = re.search(r'Address:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                code = re.search(r'Code:(.*?)</p>', str(col)) if ident_info else None\n",
    "                                date_of_listing = re.search(r'\\d{1,2}/\\d{1,2}/\\d{4}', str(col)) if ident_info else None\n",
    "\n",
    "                                # Add the extracted information to the sanction dictionary\n",
    "                                sanction_dict = {\n",
    "                                    **document_info,  # Include document-level information\n",
    "                                    'entity_type': current_entity_type,\n",
    "                                    'name_target': target_name if target_name else None,\n",
    "                                    'alias': alias if alias else None,\n",
    "                                    'reason_txt': reason if reason else None,\n",
    "                                    'position_target': position.group(1).strip() if position else None,\n",
    "                                    'date_of_birth': dob.group(1).strip() if dob else None,\n",
    "                                    'place_of_birth': pob.group(1).strip() if pob else None,\n",
    "                                    'nationality': nationality.group(1).strip() if nationality else None,\n",
    "                                    'gender': gender.group(1).strip() if gender else None,\n",
    "                                    'passport_number': passport.group(1).strip() if passport else None,\n",
    "                                    'social_media': social_media.group(1).strip() if social_media else None,\n",
    "                                    'email': email.group(1).strip() if email else None,\n",
    "                                    'telephone': telephone.group(1).strip() if telephone else None,\n",
    "                                    'address': address.group(1).strip() if address else None,\n",
    "                                    'code': code.group(1).strip() if code else None,\n",
    "                                    'date_of_listing': listing_date if listing_date else None\n",
    "                                }\n",
    "                            sanction_list.append(sanction_dict)\n",
    "\n",
    "                # Process lists (if entities are listed in bullet points)\n",
    "                elif element.name in ['ul', 'ol']:\n",
    "                    items = element.find_all('li')\n",
    "                    for item in items:\n",
    "                        item_text = item.get_text(strip=True)\n",
    "                        if item_text:\n",
    "                            # Create a dictionary for each item with the current entity type\n",
    "                            sanction_dict = {\n",
    "                                **document_info,\n",
    "                                'entity_type': current_entity_type,\n",
    "                                'name_target': item_text,\n",
    "                                'alias': None,\n",
    "                                'reason_txt': None,\n",
    "                                'position_target': None,\n",
    "                                'date_of_birth': None,\n",
    "                                'place_of_birth': None,\n",
    "                                'nationality': None,\n",
    "                                'gender': None,\n",
    "                                'passport_number': None,\n",
    "                                'social_media': None,\n",
    "                                'email': None,\n",
    "                                'telephone': None,\n",
    "                                'address': None,\n",
    "                                'code': None,\n",
    "                                'date_of_listing': None\n",
    "                            }\n",
    "                            sanction_list.append(sanction_dict)\n",
    "\n",
    "# After processing all files, sanction_list will contain all the extracted data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
