{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6cf999-032e-4899-b9e6-517384d2731b",
   "metadata": {},
   "source": [
    "# this script builds a loop over all documents to extract the beccessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205ccf91-02cb-4f6f-858f-c80f6f5901b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load all neccessary packages\n",
    "import pandas\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4efd6579-a821-4c46-a61f-0e78a580fe44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the directory of the datafiles I want to loop over\n",
    "user = '/Users/natalies_macbook/Documents/' #change this to your folder where GitHub sits\n",
    "directory = user + 'GitHub/INSA/EU/relevant'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5728f8-5688-42e9-9ddd-53ba78f6b5a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# basic pattern overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a134aa-61da-4d85-b040-c34e59525af8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#define all patterns\n",
    "#get the document base Information       \n",
    "url= r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"' #finds the url through the href tag\n",
    "celex= r'uri=CELEX%([^\"]+)' # get the celex id\n",
    "\n",
    "def extract_publication_details(soup):\n",
    "    publication_date = soup.find('p', class_='oj-hd-date').get_text(strip=True)\n",
    "    title = soup.find('p', class_='oj-doc-ti').get_text(strip=True)\n",
    "    return publication_date, title\n",
    "\n",
    "#get the document information\n",
    "decision_match = r'(COUNCIL.*?)</p'\n",
    "legal_decision = r'<p[^>]*>(.*?(amending|implementing).*?)</p>'\n",
    "legal_base = r'<p[^>]*>(.*?regard to.*?)</p>'\n",
    "case_number = r'Case\\sT-\\d+/\\d+'\n",
    "law_number = r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>'\n",
    "legal_base_alt = r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)'\n",
    "\n",
    "#check for list updates, removals or deletions\n",
    "action_intended = r'<p class=\"(oj-normal|oj-bold|normal)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced)\\s*.*?)\\s*</p>'\n",
    "\n",
    "#for annex with numbers\n",
    "listing_position = r'class=\"oj-normal\">(\\d+.*?)'\n",
    "name_2 = r'<p class=\"oj-normal\">\\s*<span class=\"oj-bold\">(.*?)</span>'\n",
    "un_sanction_date = r'class=\"oj-normal\">Date of UN designation(.*?)</p>'\n",
    "identifying_information = r'>(.*?Date of Birth:.*?)</p>'\n",
    "\n",
    "UN_pattern = r'UN'\n",
    "\n",
    "#court decision matching\n",
    "name_pattern_court = r'Orders (.+?) to pay'\n",
    "#for dismissal entity name\n",
    "\n",
    "delist_pattern = r'Annuls'\n",
    "delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "name_pattern1 = r'Orders (.+?) to pay'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bad5a-22af-41e7-b186-b61b37af3953",
   "metadata": {
    "tags": []
   },
   "source": [
    "# patterns for court decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a8966596-63f6-497c-a6b4-e4d92d2b20b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "court_decision_pattern = r'<p class=\"oj-doc-ti\".*?>Judgment of the General Court</p>'\n",
    "\n",
    "def extract_court_decision(html_content):\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Define the patterns\n",
    "    delist_pattern = r'Annuls'\n",
    "    delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "    name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "    name_pattern1 = r'Orders (.+?) to pay'\n",
    "    \n",
    "    # Initialize variables\n",
    "    Delisting = 0  # Default to 0 (not a delisting)\n",
    "    delisting_case = None\n",
    "    entity_name = None\n",
    "    \n",
    "    # Extract the main text content from the relevant <p> tags\n",
    "    paragraphs = soup.find_all('p', class_='oj-normal')\n",
    "    text = ' '.join(p.get_text() for p in paragraphs)\n",
    "    \n",
    "    # Check for delisting\n",
    "    if re.search(delist_pattern, text):\n",
    "        Delisting = 1  # Set to 1 if it matches the delisting pattern\n",
    "        match = re.search(name_pattern2, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    else:\n",
    "        match = re.search(name_pattern1, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    \n",
    "    # Extract delisting case\n",
    "    match = re.search(delisting_case_pattern, text)\n",
    "    if match:\n",
    "        delisting_case = match.group(1)\n",
    "    \n",
    "    # Extract additional details using BeautifulSoup and regex\n",
    "    return {\n",
    "        'entity_name': entity_name,\n",
    "        'Delisting': Delisting,  # Binary indicator (1 for delisting, 0 otherwise)\n",
    "        'Delisting_case': delisting_case,\n",
    "        'url': [a['href'] for a in soup.find_all('a', href=True) if '.eu/legal-content/EN/TXT/HTML/' in a['href']],\n",
    "        'celex': re.findall(r'uri=CELEX%([^\"]+)', str(soup)),\n",
    "        'publication_date': re.findall(r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}', str(soup.find('p', class_='oj-hd-date'))),\n",
    "        'date': re.findall(r'of (\\d+ \\w+ \\d+)', str(soup.find('p', class_='oj-doc-ti'))),\n",
    "        'title': [t.get_text() for t in soup.find_all('p', class_='oj-doc-ti')],\n",
    "        'law_number': re.findall(r'C (\\d+/\\d+)', str(soup.find('p', class_='oj-hd-oj'))),\n",
    "        'case_number': re.findall(r'Case\\sT-\\d+/\\d+', str(soup)),\n",
    "        'legal_base': re.findall(r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)', str(soup))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa6236-9ccb-41b7-9768-8bd32d609f67",
   "metadata": {
    "tags": []
   },
   "source": [
    "def extract_court_decision(data):\n",
    "    # Define the patterns\n",
    "    delist_pattern = r'Annuls'\n",
    "    delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "    name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "    name_pattern1 = r'Orders (.+?) to pay'\n",
    "    \n",
    "    # Initialize variables\n",
    "    Delisting = 0  # Default to 0 (not a delisting)\n",
    "    delisting_case = None\n",
    "    entity_name = None\n",
    "    \n",
    "    # Get the content of the court case\n",
    "    name = re.findall(r'<p class=\"oj-normal\">(.*?)</p>', data)\n",
    "    text = ''.join(name)\n",
    "    \n",
    "    # Check for delisting\n",
    "    if re.search(delist_pattern, text):\n",
    "        Delisting = 1  # Set to 1 if it matches the delisting pattern\n",
    "        match = re.search(name_pattern2, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    else:\n",
    "        match = re.search(name_pattern1, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    \n",
    "    # Extract delisting case\n",
    "    match = re.search(delisting_case_pattern, text)\n",
    "    if match:\n",
    "        delisting_case = match.group(1)\n",
    "    \n",
    "    # Create a dictionary for the current court decision\n",
    "    return {\n",
    "        'entity_name': entity_name,\n",
    "        'Delisting': Delisting,  # Binary indicator (1 for delisting, 0 otherwise)\n",
    "        'Delisting_case': delisting_case,\n",
    "        'url': re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data),\n",
    "        'celex': re.findall(r'uri=CELEX%([^\"]+)', data),\n",
    "        'publication_date': re.findall(r'<p class=\"oj-hd-date\">(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', data),\n",
    "        'date': re.findall(r'<p class=\"oj-doc-ti\">of (\\d+ \\w+ \\d+)</p>', data),\n",
    "        'title': re.findall(r'<p class=\"oj-doc-ti\" id=\"[^\"]*\">(.*?)</p>', data),\n",
    "        'law_number': re.findall(r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>', data),\n",
    "        'case_number': re.findall(r'Case\\sT-\\d+/\\d+', data),\n",
    "        'legal_base': re.findall(r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)', data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ed5a9-1eb7-4473-8722-ae6833d1e0a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# patterns for notices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b533fead-0ac8-4fa2-a34a-351810824a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "notice_pattern = r'<p class=\"oj-doc-ti\".*?\">Notice.*?</p>'\n",
    "\n",
    "def notice_pattern_extraction(soup):\n",
    "    # Document base information\n",
    "    url_pattern = r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"'\n",
    "    celex_pattern = r'uri=CELEX%([^\"]+)'\n",
    "    \n",
    "    # Extract publication date and title\n",
    "    publication_date = soup.find('p', class_='oj-hd-date')\n",
    "    publication_date = publication_date.get_text(strip=True) if publication_date else None\n",
    "    \n",
    "    title = soup.find('p', class_='oj-doc-ti')\n",
    "    title = title.get_text(strip=True) if title else None\n",
    "    # turn soup into string\n",
    "    soup_str = str(soup)\n",
    "    # Extract URL and CELEX\n",
    "    url_match = re.search(url_pattern, soup_str)\n",
    "    celex_match = re.search(celex_pattern, soup_str)\n",
    "    url = url_match.group(1) if url_match else None\n",
    "    celex = celex_match.group(1) if celex_match else None\n",
    "    \n",
    "    # Document information patterns\n",
    "    decision_match_pattern = r'(Decision[^,]*|Regulation[^,]*|Annex[^,]*)'\n",
    "    legal_action_pattern = r'<p[^>]*>(.*?(implementing|amended|implemented).*?)</p>'\n",
    "    legal_base_pattern = r'<p[^>]*>(.*?regard to|amending.*?)</p>'\n",
    "    case_number_pattern = r'Case\\sT-\\d+/\\d+'\n",
    "    law_number_pattern = r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>'\n",
    "    legal_base_alt_pattern = r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)'\n",
    "    \n",
    "    decision_match = re.search(decision_match_pattern, soup_str)\n",
    "    legal_action = re.search(legal_action_pattern, soup_str)\n",
    "    legal_base = re.search(legal_base_pattern, soup_str)\n",
    "    case_number = re.search(case_number_pattern, soup_str)\n",
    "    law_number = re.search(law_number_pattern, soup_str)\n",
    "    legal_base_alt = re.search(legal_base_alt_pattern, soup_str)\n",
    "    \n",
    "    # List updates, removals, deletions\n",
    "    action_intended_pattern = r'<p class=\"(oj-normal|oj-bold|normal|bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced|amended)\\s*.*?)\\s*</p>'\n",
    "    action_intended = re.search(action_intended_pattern, str(soup))\n",
    "    notice_target = re.search(r'The following information is brought to the attention of\\s*(.*?),', soup_str)\n",
    "    \n",
    "    return {'publication_date': publication_date,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "            'celex': celex,\n",
    "            'decision_match': decision_match.group(1) if decision_match else None,\n",
    "            'legal_action': legal_action.group(1) if legal_action else None,\n",
    "            'legal_base': legal_base.group(1) if legal_base else None,\n",
    "            'case_number': case_number.group(0) if case_number else None,\n",
    "            'law_number': law_number.group(1) if law_number else None,\n",
    "            'legal_base_alt': legal_base_alt.group(0) if legal_base_alt else None,\n",
    "            'action_intended': action_intended.group(2) if action_intended else None,\n",
    "            'notice_target':notice_target.group(1) if notice_target else None}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8fa8e-99e9-4144-9b26-76398bb1e871",
   "metadata": {},
   "source": [
    "# main pattern all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cd1a5265-51a8-4e55-91d4-e51cc78565f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully memorized the listing patterns\n"
     ]
    }
   ],
   "source": [
    "#paterns for the general listing documents\n",
    "def extract_document_info(soup):\n",
    "    # Document base information\n",
    "    url_pattern = r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"'\n",
    "    celex_pattern = r'uri=CELEX%([^\"]+)'\n",
    "    \n",
    "    # Extract publication date and title\n",
    "    publication_date = soup.find('p', class_='oj-hd-date')\n",
    "    publication_date = publication_date.get_text(strip=True) if publication_date else None\n",
    "    \n",
    "    title = soup.find('p', class_='oj-doc-ti')\n",
    "    title = title.get_text(strip=True) if title else None\n",
    "    # turn soup into string\n",
    "    soup_str = str(soup)\n",
    "    # Extract URL and CELEX\n",
    "    url_match = re.search(url_pattern, soup_str)\n",
    "    celex_match = re.search(celex_pattern, soup_str)\n",
    "    url = url_match.group(1) if url_match else None\n",
    "    celex = celex_match.group(1) if celex_match else None\n",
    "    \n",
    "    # Document information patterns\n",
    "    decision_match_pattern = r'(Decision[^,]*|Regulation[^,]*|Annex[^,]*)'\n",
    "    legal_action_pattern = r'<p[^>]*>(.*?(implementing|amended|implemented).*?)</p>'\n",
    "    legal_base_pattern = r'<p[^>]*>(.*?regard to|amending.*?)</p>'\n",
    "    case_number_pattern = r'Case\\sT-\\d+/\\d+'\n",
    "    law_number_pattern = r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>'\n",
    "    legal_base_alt_pattern = r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)'\n",
    "    \n",
    "    decision_match = re.search(decision_match_pattern, soup_str)\n",
    "    legal_action = re.search(legal_action_pattern, soup_str)\n",
    "    legal_base = re.search(legal_base_pattern, soup_str)\n",
    "    case_number = re.search(case_number_pattern, soup_str)\n",
    "    law_number = re.search(law_number_pattern, soup_str)\n",
    "    legal_base_alt = re.search(legal_base_alt_pattern, soup_str)\n",
    "    \n",
    "    # List updates, removals, deletions\n",
    "    action_intended_pattern = r'<p class=\"(oj-normal|oj-bold|normal|bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced|amended)\\s*.*?)\\s*</p>'\n",
    "    action_intended = re.search(action_intended_pattern, str(soup))\n",
    "    \n",
    "    # Annex with numbers\n",
    "    listing_position_pattern = r'class=\"oj-normal|normal\">(\\d+.*?)'\n",
    "    name_2_pattern = r'<p class=\"oj-normal|normal\">\\s*<span class=\"oj-bold|bold\">(.*?)</span>'\n",
    "    un_sanction_date_pattern = r'class=\"oj-normal|normal\">Date of UN designation(.*?)</p>'\n",
    "    identifying_information_pattern = r'>(.*?Date of Birth:.*?)</p>'\n",
    "    \n",
    "    listing_position = re.search(listing_position_pattern, soup_str)\n",
    "    name_2 = re.search(name_2_pattern, soup_str)\n",
    "    un_sanction_date = re.search(un_sanction_date_pattern, soup_str)\n",
    "    un_sanction = '1' if un_sanction_date else '0'\n",
    "    identifying_information = re.search(identifying_information_pattern, soup_str) #also fix this here!\n",
    "    \n",
    "    # Court decision matching\n",
    "    #name_pattern_court = r'Orders (.+?) to pay'\n",
    "    #delist_pattern = r'Annuls'\n",
    "    #delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "    #name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "    #name_pattern1 = r'Orders (.+?) to pay'\n",
    "\n",
    "    #name_court = re.search(name_pattern_court, str(soup))\n",
    "    #delist_match = re.search(delist_pattern, str(soup))\n",
    "    #delisting_case = re.search(delisting_case_pattern, str(soup))\n",
    "    #name_case2 = re.search(name_pattern2, str(soup))\n",
    "    #name_case1 = re.search(name_pattern1, str(soup))\n",
    "    \n",
    "    # Return all extracted information\n",
    "    return {\n",
    "        'publication_date': publication_date,\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'celex': celex,\n",
    "        'decision_match': decision_match.group(1) if decision_match else None,\n",
    "        'legal_action': legal_action.group(1) if legal_action else None,\n",
    "        'legal_base': legal_base.group(1) if legal_base else None,\n",
    "        'case_number': case_number.group(0) if case_number else None,\n",
    "        'law_number': law_number.group(1) if law_number else None,\n",
    "        'legal_base_alt': legal_base_alt.group(0) if legal_base_alt else None,\n",
    "        'action_intended': action_intended.group(2) if action_intended else None,\n",
    "        'listing_position': listing_position.group(1) if listing_position else None,\n",
    "        'name_2': name_2.group(1) if name_2 else None,\n",
    "        'un_sanction_date': un_sanction_date.group(1) if un_sanction_date else None,\n",
    "        'un_sanction': un_sanction,\n",
    "        'identifying_information': identifying_information.group(1) if identifying_information else None,\n",
    "        #'name_court': name_court.group(1) if name_court else None,\n",
    "        #'delist_match': delist_match.group(0) if delist_match else None,\n",
    "        #'delisting_case': delisting_case.group(1) if delisting_case else None,\n",
    "        #'name_case2': name_case2.group(1) if name_case2 else None,\n",
    "        #'name_case1': name_case1.group(1) if name_case1 else None,\n",
    "    }\n",
    "print('successfully memorized the listing patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "962dd8c9-ce6a-4b0e-8bc4-e08e93c28d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "notice_list = []\n",
    "sanction_list = []\n",
    "court_decision_list = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "        \n",
    "        # Parse the file content with BeautifulSoup\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        \n",
    "        # Extract document-level information\n",
    "        document_info = extract_document_info(soup)\n",
    "        \n",
    "        # Check if the document matches a court decision pattern\n",
    "        if re.findall(court_decision_pattern, str(soup)):\n",
    "            court_info = extract_court_decision(soup)\n",
    "            court_decision_list.append(court_info)\n",
    "        \n",
    "        # Check if the document matches a notice pattern\n",
    "        if re.findall(notice_pattern, str(soup)):\n",
    "            notice_info = notice_pattern_extraction(soup)\n",
    "            notice_list.append(notice_info)\n",
    "        \n",
    "        # Find all table rows of the class 'oj-table' and extract relevant information\n",
    "        rows = soup.find_all('tr', class_='oj-table')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            \n",
    "            # Extract name, alias, and other relevant details\n",
    "            \n",
    "            if cols:\n",
    "                for col in cols:\n",
    "                    header_text = col.get('data-header')  # Replace 'data-header' with the actual attribute or use another method\n",
    "            \n",
    "            if header_text == 'Name':\n",
    "                entity_info['name'] = col.get_text(strip=True)\n",
    "            elif header_text == 'Reasons':\n",
    "                entity_info['reason'] = col.get_text(strip=True)\n",
    "            elif header_text == 'Identifying Information':\n",
    "                entity_info['ident_info'] = col.get_text(strip=True)\n",
    "            elif header_text == 'Date of Listing':\n",
    "                listing_date[''] = col.get_text(strip=True)\n",
    "            # Extract information using regular expressions\n",
    "            position = re.search(r'Position\\(s\\):(.*?)</p>', str(row))\n",
    "            dob = re.search(r'(?:DOB|Date of Birth|Born on): (\\d{1,2}/\\d{1,2}/\\d{4})', str(row))\n",
    "            pob = re.search(r'POB|Place of Birth:(.*?)</p>', str(row))\n",
    "            nationality = re.search(r'Nationality:(.*?)</p>', str(row))\n",
    "            gender = re.search(r'Gender:(.*?)</p>', str(row))\n",
    "            passport = re.search(r'Passport Number:(.*?)</p>', str(row))\n",
    "            social_media = re.search(r'Social media:(.*?)</p>', str(row))\n",
    "            email = re.search(r'Email:(.*?)</p>', str(row))\n",
    "            telephone = re.search(r'Telephone:(.*?)</p>', str(row))\n",
    "            address = re.search(r'Address:(.*?)</p>', str(row))\n",
    "            code = re.search(r'Code:(.*?)</p>', str(row))\n",
    "            date_of_listing = re.search(r'\\d{1,2}/\\d{1,2}/\\d{4}', str(row))\n",
    "            \n",
    "            # Combine document_info with row-specific information into a dictionary\n",
    "            sanction_dict = {\n",
    "                **document_info,\n",
    "                'name_target': name,\n",
    "                'alias': alias,\n",
    "                'position_target': position.group(1) if position else None,\n",
    "                'date_of_birth': dob.group(1) if dob else None,\n",
    "                'place_of_birth': pob.group(1) if pob else None,\n",
    "                'nationality': nationality.group(1) if nationality else None,\n",
    "                'gender': gender.group(1) if gender else None,\n",
    "                'passport_number': passport.group(1) if passport else None,\n",
    "                'social_media': social_media.group(1) if social_media else None,\n",
    "                'email': email.group(1) if email else None,\n",
    "                'telephone': telephone.group(1) if telephone else None,\n",
    "                'address': address.group(1) if address else None,\n",
    "                'code': code.group(1) if code else None,\n",
    "                'date_of_listing': date_of_listing.group(0) if date_of_listing else None\n",
    "            }\n",
    "            \n",
    "            sanction_list.append(sanction_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ca45733e-779b-44d8-902d-df419a7c7104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "court_decision_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e9821-ad68-4fbe-829e-a1d448efb942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through all files in the directory\n",
    "ident_info = None\n",
    "reason = None\n",
    "listing_date = None\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "        \n",
    "        # Parse the file content with BeautifulSoup\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        \n",
    "        # Extract document-level information\n",
    "        document_info = extract_document_info(soup)\n",
    "        \n",
    "        # Check if the document matches a court decision pattern\n",
    "        if re.findall(court_decision_pattern, str(soup)):\n",
    "            court_info = extract_court_decision(soup)\n",
    "            court_decision_list.append(court_info)\n",
    "        \n",
    "        # Check if the document matches a notice pattern\n",
    "        if re.findall(notice_pattern, str(soup)):\n",
    "            notice_info = notice_pattern_extraction(soup)\n",
    "            notice_list.append(notice_info)\n",
    "        \n",
    "        # Find all table rows of the class 'oj-table' and extract relevant information\n",
    "        rows = soup.find_all('tr', class_='oj-table')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                for col in cols:\n",
    "                    header_text = col.get('data-header')  # Replace 'data-header' with the actual attribute or use another method\n",
    "                    \n",
    "                    if header_text == 'Name':\n",
    "                        target_name = col.get_text(strip=True)\n",
    "                    elif header_text == 'Reasons':\n",
    "                        reason = col.get_text(strip=True)\n",
    "                    elif header_text == 'Identifying Information':\n",
    "                        ident_info = col.get_text(strip=True)\n",
    "                    elif header_text == 'Date of Listing':\n",
    "                        listing_date = col.get_text(strip=True)\n",
    "\n",
    "            # Extract additional information using regular expressions\n",
    "                    position = re.search(r'Position\\(s\\):(.*?)</p>', ident_info) if ident_info else None\n",
    "                    dob = re.search(r'(?:DOB|Date of Birth|Born on): (\\d{1,2}/\\d{1,2}/\\d{4})', ident_info) if ident_info else None\n",
    "                    pob = re.search(r'POB|Place of Birth:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    nationality = re.search(r'Nationality:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    gender = re.search(r'Gender:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    passport = re.search(r'Passport Number:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    social_media = re.search(r'Social media:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    email = re.search(r'Email:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    telephone = re.search(r'Telephone:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    address = re.search(r'Address:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    code = re.search(r'Code:(.*?)</p>', ident_info)if ident_info else None\n",
    "                    date_of_listing = re.search(r'\\d{1,2}/\\d{1,2}/\\d{4}', ident_info)if ident_info else None\n",
    "            \n",
    "            # Combine document_info with row-specific information into a dictionary\n",
    "            sanction_dict = {\n",
    "                **document_info,  # Include document-level information\n",
    "                'name_target': name if name else None,\n",
    "                'alias': alias if alias else None,\n",
    "                'reason_txt':reason if reason else None,\n",
    "                'position_target': position if position else None,\n",
    "                'date_of_birth': dob if dob else None,\n",
    "                'place_of_birth': pob if pob else None,\n",
    "                'nationality': nationality if nationality else None,\n",
    "                'gender': gender if gender else None,\n",
    "                'passport_number': passport if passport else None,\n",
    "                'social_media': social_media if social_media else None,\n",
    "                'email': email if email else None,\n",
    "                'telephone': telephone if telephone else None,\n",
    "                'address': address if address else None,\n",
    "                'code': code if code else None,\n",
    "                'date_of_listing': listing_date if listing_date else None\n",
    "            }\n",
    "            sanction_list.append(sanction_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa83ab-5315-4b72-b8db-56ad119e25e0",
   "metadata": {},
   "source": [
    "# pickling the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ddcd533-e0de-4375-9556-5313828fd2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1f429-ae81-4618-b201-0e0af46a3b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"batched_sanction_EU\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(sanction_list, fp)\n",
    "\n",
    "with open(\"batched_notice_EU\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(notice_list, fp)\n",
    "\n",
    "with open(\"batched_court_decision_EU\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(court_decision_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef7bf9-0535-4cb6-9d48-a0b855a85fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with open(\"batched_sanction_EU\", \"rb\") as fp:   #Pickling\n",
    "#    pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
