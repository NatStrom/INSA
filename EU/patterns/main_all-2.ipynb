{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6cf999-032e-4899-b9e6-517384d2731b",
   "metadata": {},
   "source": [
    "# this script builds a loop over all documents to extract the beccessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205ccf91-02cb-4f6f-858f-c80f6f5901b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load all neccessary packages\n",
    "import pandas\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4efd6579-a821-4c46-a61f-0e78a580fe44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the directory of the datafiles I want to loop over\n",
    "user = '/Users/natalies_macbook/Documents/' #change this to your folder where GitHub sits\n",
    "directory = user + 'GitHub/INSA/EU/relevant'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5728f8-5688-42e9-9ddd-53ba78f6b5a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# basic pattern overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a134aa-61da-4d85-b040-c34e59525af8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#define all patterns\n",
    "#get the document base Information       \n",
    "url= r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"' #finds the url through the href tag\n",
    "celex= r'uri=CELEX%([^\"]+)' # get the celex id\n",
    "\n",
    "def extract_publication_details(soup):\n",
    "    publication_date = soup.find('p', class_='oj-hd-date').get_text(strip=True)\n",
    "    title = soup.find('p', class_='oj-doc-ti').get_text(strip=True)\n",
    "    return publication_date, title\n",
    "\n",
    "#get the document information\n",
    "decision_match = r'(COUNCIL.*?)</p'\n",
    "legal_decision = r'<p[^>]*>(.*?(amending|implementing).*?)</p>'\n",
    "legal_base = r'<p[^>]*>(.*?regard to.*?)</p>'\n",
    "case_number = r'Case\\sT-\\d+/\\d+'\n",
    "law_number = r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>'\n",
    "legal_base_alt = r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)'\n",
    "\n",
    "#check for list updates, removals or deletions\n",
    "action_intended = r'<p class=\"(oj-normal|oj-bold|normal)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced)\\s*.*?)\\s*</p>'\n",
    "\n",
    "#for annex with numbers\n",
    "listing_position = r'class=\"oj-normal\">(\\d+.*?)'\n",
    "name_2 = r'<p class=\"oj-normal\">\\s*<span class=\"oj-bold\">(.*?)</span>'\n",
    "un_sanction_date = r'class=\"oj-normal\">Date of UN designation(.*?)</p>'\n",
    "identifying_information = r'>(.*?Date of Birth:.*?)</p>'\n",
    "\n",
    "UN_pattern = r'UN'\n",
    "\n",
    "#court decision matching\n",
    "name_pattern_court = r'Orders (.+?) to pay'\n",
    "#for dismissal entity name\n",
    "\n",
    "delist_pattern = r'Annuls'\n",
    "delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "name_pattern1 = r'Orders (.+?) to pay'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bad5a-22af-41e7-b186-b61b37af3953",
   "metadata": {
    "tags": []
   },
   "source": [
    "# court decision matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39eee05c-165c-4d18-8523-b0e3a519ab27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m court_decisions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Iterate over the content\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Get the content of the court case\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     name \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<p class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moj-normal\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>(.*?)</p>\u001b[39m\u001b[38;5;124m'\u001b[39m, i)\n\u001b[1;32m     17\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "#court decision matching\n",
    "name_pattern_court = r'Orders (.+?) to pay'\n",
    "#for dismissal entity name\n",
    "# Create patterns\n",
    "delist_pattern = r'Annuls'\n",
    "delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "name_pattern1 = r'Orders (.+?) to pay'\n",
    "\n",
    "# Create a list to store all court decisions\n",
    "court_decisions = []\n",
    "\n",
    "# Iterate over the content\n",
    "for i in content:\n",
    "    # Get the content of the court case\n",
    "    name = re.findall(r'<p class=\"oj-normal\">(.*?)</p>', i)\n",
    "    text = ''.join(name)\n",
    "    \n",
    "    # Initialize variables\n",
    "    Delisting = None\n",
    "    delisting_case = None\n",
    "    entity_name = None\n",
    "    \n",
    "    # Check for delisting\n",
    "    if re.search(delist_pattern, text):\n",
    "        Delisting = 1\n",
    "    else:\n",
    "        Delisting = 0\n",
    "    if Delisting == 1:\n",
    "        match = re.search(name_pattern2, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    if Delisting == 0:\n",
    "        match = re.search(name_pattern1, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    \n",
    "    # Extract delisting case\n",
    "    match = re.search(delisting_case_pattern, text)\n",
    "    if match:\n",
    "        delisting_case = match.group(1)\n",
    "\n",
    "    # Create dictionary for current court decision\n",
    "    court_decision = {\n",
    "        'entity_name': entity_name,\n",
    "        'Delisting': Delisting,\n",
    "        'Delisting_case': delisting_case,\n",
    "        'url': re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data),\n",
    "        'celex' : re.findall(r'uri=CELEX%([^\"]+)', data),\n",
    "        'publication_date': re.findall(r'<p class=\"oj-hd-date\">(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', data),\n",
    "        'date': re.findall(r'<p class=\"oj-doc-ti\">of (\\d+ \\w+ \\d+)</p>', data),\n",
    "        'title': re.findall(r'<p class=\"oj-doc-ti\" id=\"[^\"]*\">(.*?)</p>', data),\n",
    "        'law_number': re.findall(r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>', data),\n",
    "        'case_number': re.findall(r'Case\\sT-\\d+/\\d+', data),\n",
    "        'legal_base': re.findall(r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)', data)\n",
    "    }\n",
    "    \n",
    "    # Append the current decision dictionary to the list\n",
    "    court_decisions.append(court_decision)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(court_decisions, encoding = 'UTF-8') #encoding Ã¤ndern oder string subsitute (string punkt replace xa0 mit .)\n",
    "#now extract th information\n",
    "# Initialize variables\n",
    "Delisting = None\n",
    "delisting_case = None\n",
    "entity_name = None\n",
    "#create patterns\n",
    "delist_pattern = r'The Court: 1. Annuls'\n",
    "delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "name_pattern = r'in so far as they concern (.+?);2.Orders'\n",
    "#create empty dic\n",
    "results = {}\n",
    "\n",
    "for i in content:\n",
    "    #get the content of the court case\n",
    "    name = re.findall(r'<p class=\"oj-normal\">(.*?)</p>', i)\n",
    "    text = ''.join(name)\n",
    "    if re.search(delist_pattern, text):\n",
    "        Delisting = 0\n",
    "        print('Dismissal')\n",
    "    else:\n",
    "        Delisting = 1  # Assuming 1 for non-dismissal cases\n",
    "    match = re.search(delisting_case_pattern, text)\n",
    "    if match:\n",
    "        delisting_case = match.group(1)\n",
    "    # Extract entity name\n",
    "    match = re.search(name_pattern, text)\n",
    "    if match:\n",
    "        entity_name = match.group(1)\n",
    "#now get it into a dictionary format\n",
    "court_decision = {}\n",
    "court_decision['entity_name'] = entity_name\n",
    "court_decision['Delisting'] = Delisting\n",
    "court_decision['Delisting_case']= delisting_case\n",
    "court_decision['url'] = re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data)\n",
    "court_decision['publication_date'] =re.findall(r'<p class=\"oj-hd-date\">(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', data)\n",
    "court_decision['date'] =  re.findall(r'<p class=\"oj-doc-ti\">of (\\d+ \\w+ \\d+)</p>', data)\n",
    "court_decision['title']= re.findall(r'<p class=\"oj-doc-ti\" id=\"[^\"]*\">(.*?)</p>', data)\n",
    "court_decision['law_number'] = re.findall(r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)',data)\n",
    "court_decision['case_number'] = re.findall(r'Case\\sT-\\d+/\\d+', data)\n",
    "court_decision['legal_base'] = re.findall(r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61f525-c981-44b5-a582-7d85ad84a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_court_decision(data):\n",
    "    # Define the patterns\n",
    "    delist_pattern = r'Annuls'\n",
    "    delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "    name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "    name_pattern1 = r'Orders (.+?) to pay'\n",
    "    \n",
    "    # Initialize variables\n",
    "    Delisting = 0  # Default to 0 (not a delisting)\n",
    "    delisting_case = None\n",
    "    entity_name = None\n",
    "    \n",
    "    # Get the content of the court case\n",
    "    name = re.findall(r'<p class=\"oj-normal\">(.*?)</p>', data)\n",
    "    text = ''.join(name)\n",
    "    \n",
    "    # Check for delisting\n",
    "    if re.search(delist_pattern, text):\n",
    "        Delisting = 1  # Set to 1 if it matches the delisting pattern\n",
    "        match = re.search(name_pattern2, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    else:\n",
    "        match = re.search(name_pattern1, text)\n",
    "        if match:\n",
    "            entity_name = match.group(1)\n",
    "    \n",
    "    # Extract delisting case\n",
    "    match = re.search(delisting_case_pattern, text)\n",
    "    if match:\n",
    "        delisting_case = match.group(1)\n",
    "    \n",
    "    # Create a dictionary for the current court decision\n",
    "    return {\n",
    "        'entity_name': entity_name,\n",
    "        'Delisting': Delisting,  # Binary indicator (1 for delisting, 0 otherwise)\n",
    "        'Delisting_case': delisting_case,\n",
    "        'url': re.findall(r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"', data),\n",
    "        'celex': re.findall(r'uri=CELEX%([^\"]+)', data),\n",
    "        'publication_date': re.findall(r'<p class=\"oj-hd-date\">(\\d{1,2}\\.\\d{1,2}\\.\\d{4})', data),\n",
    "        'date': re.findall(r'<p class=\"oj-doc-ti\">of (\\d+ \\w+ \\d+)</p>', data),\n",
    "        'title': re.findall(r'<p class=\"oj-doc-ti\" id=\"[^\"]*\">(.*?)</p>', data),\n",
    "        'law_number': re.findall(r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>', data),\n",
    "        'case_number': re.findall(r'Case\\sT-\\d+/\\d+', data),\n",
    "        'legal_base': re.findall(r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)', data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ed5a9-1eb7-4473-8722-ae6833d1e0a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# patterns for notices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533fead-0ac8-4fa2-a34a-351810824a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notice_pattern_extraction(soup):\n",
    "    # Document base information\n",
    "    url_pattern = r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"'\n",
    "    celex_pattern = r'uri=CELEX%([^\"]+)'\n",
    "    \n",
    "    # Extract publication date and title\n",
    "    publication_date = soup.find('p', class_='oj-hd-date')\n",
    "    publication_date = publication_date.get_text(strip=True) if publication_date else None\n",
    "    \n",
    "    title = soup.find('p', class_='oj-doc-ti')\n",
    "    title = title.get_text(strip=True) if title else None\n",
    "    \n",
    "    # Extract URL and CELEX\n",
    "    url_match = re.search(url_pattern, str(soup))\n",
    "    celex_match = re.search(celex_pattern, str(soup))\n",
    "    url = url_match.group(1) if url_match else None\n",
    "    celex = celex_match.group(1) if celex_match else None\n",
    "    \n",
    "    # Document information patterns\n",
    "    decision_match_pattern = r'(Decision[^,]*|Regulation[^,]*|Annex[^,]*)'\n",
    "    legal_action_pattern = r'<p[^>]*>(.*?(implementing|amended|implemented).*?)</p>'\n",
    "    legal_base_pattern = r'<p[^>]*>(.*?regard to|amending.*?)</p>'\n",
    "    case_number_pattern = r'Case\\sT-\\d+/\\d+'\n",
    "    law_number_pattern = r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>'\n",
    "    legal_base_alt_pattern = r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)'\n",
    "    \n",
    "    decision_match = re.search(decision_match_pattern, str(soup))\n",
    "    legal_decision = re.search(legal_decision_pattern, str(soup))\n",
    "    legal_base = re.search(legal_base_pattern, str(soup))\n",
    "    case_number = re.search(case_number_pattern, str(soup))\n",
    "    law_number = re.search(law_number_pattern, str(soup))\n",
    "    legal_base_alt = re.search(legal_base_alt_pattern, str(soup))\n",
    "    \n",
    "    # List updates, removals, deletions\n",
    "    action_intended_pattern = r'<p class=\"(oj-normal|oj-bold|normal|bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced|amended)\\s*.*?)\\s*</p>'\n",
    "    action_intended = re.search(action_intended_pattern, str(soup))\n",
    "    notice_target = re.findall(r'The following information is brought to the attention of\\s*(.*?),', str(soup))\n",
    "    \n",
    "    return {'publication_date': publication_date,\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'celex': celex,\n",
    "        'decision_match': decision_match.group(1) if decision_match else None,\n",
    "        'legal_decision': legal_decision.group(1) if legal_decision else None,\n",
    "        'legal_base': legal_base.group(1) if legal_base else None,\n",
    "        'case_number': case_number.group(0) if case_number else None,\n",
    "        'law_number': law_number.group(1) if law_number else None,\n",
    "        'legal_base_alt': legal_base_alt.group(0) if legal_base_alt else None,\n",
    "        'action_intended': action_intended.group(2) if action_intended else None,\n",
    "        'notice_target':notice_target.group(1) if notice_target else None}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8fa8e-99e9-4144-9b26-76398bb1e871",
   "metadata": {},
   "source": [
    "# main pattern all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba684423-9e7d-4c29-8422-57b1a920afe2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "this one works so far but i have to include the number of the notice if it is a notice:\n",
    "    oj-no-doc-c\n",
    "    \n",
    "\n",
    "as well as fix the identifying info list, i cant pluck from the position of the returned matches\n",
    "\n",
    "this is what it looks like:\n",
    "\n",
    "                     </td>\n",
    "                     <td valign=\"top\" class=\"oj-table\">\n",
    "                        <p class=\"oj-tbl-txt\">2Â March 2021</p>\n",
    "                     </td>\n",
    "                  </tr>\n",
    "                  <tr class=\"oj-table\">\n",
    "                     <td valign=\"top\" class=\"oj-table\">\n",
    "                        <p class=\"oj-tbl-txt\">3.</p>\n",
    "                     </td>\n",
    "                     <td valign=\"top\" class=\"oj-table\">\n",
    "                        <p class=\"oj-tbl-txt\">Igor Viktorovich KRASNOV</p>\n",
    "                     </td>\n",
    "                     <td valign=\"top\" class=\"oj-table\">\n",
    "                        <p class=\"oj-tbl-txt\">ÐÐ³Ð¾ÑÑ ÐÐ¸ÐºÑÐ¾ÑÐ¾Ð²Ð¸Ñ ÐÐ ÐÐ¡ÐÐÐ</p>\n",
    "                     </td>\n",
    "                     <td valign=\"top\" class=\"oj-table\">\n",
    "                        <p class=\"oj-tbl-txt\">Position(s): Prosecutor General of the Russian Federation</p>\n",
    "                        <p class=\"oj-tbl-txt\">DOB: 24.12.1975</p>\n",
    "                        <p class=\"oj-tbl-txt\">POB: Arkhangelsk, Russian SFSR (now Russian Federation)</p>\n",
    "                        <p class=\"oj-tbl-txt\">Nationality: Russian</p>\n",
    "                        <p class=\"oj-tbl-txt\">Gender: male</p>\n",
    "                     </td>\n",
    "                     <td valign=\"top\" class=\"oj-table\">\n",
    "                        <p class=\"oj-tbl-txt\">Igor Krasnov has been Prosecutor General of the Russian Federation sinceÂ 22Â JanuaryÂ 2020, and is the former Deputy Chairman of the Investigative Committee of the Russian Federation. In his position as Prosecutor General, he supervises the Prosecutorâs Offices in the Russian Federation, the Special Prosecutorâs Offices and the Military Prosecutorâs Office. In his capacity as Prosecutor General, he is responsible for serious human rights violations, including the arbitrary detentions of protesters, and for widespread and systematic repression of freedom of peaceful assembly and of association, and freedom of opinion and expression.</p>\n",
    "                        <p class=\"oj-tbl-txt\">Ahead of theÂ 23Â JanuaryÂ 2021 protests, the Prosecutor Generalâs Office warned that participants would be held responsible. Moreover, the Prosecutor Generalâs Office demanded that the Federal Service for Supervision in the Sphere of Communications, Information Technology and Mass Communications (Roskomnadzor) restrict access to opposition websites and social network accounts that contained information on planned gatherings of Alexei Navalnyâs supporters. OnÂ 29Â JanuaryÂ 2021, the Prosecutor Generalâs Office once again demanded that Roskomnadzor restrict access to opposition websites and social network accounts, this time ahead of the pro-Navalny protests onÂ 30 andÂ 31Â JanuaryÂ 2021. Warnings were sent to internet companies (Facebook, TikTok, Twitter, Google, Mail.ru Group). The Prosecutor Generalâs Office also announced that those taking part in the demonstrations would be prosecuted.</p>\n",
    "                        <p class=\"oj-tbl-txt\">The Prosecutor Generalâs Office supported the request by the Russian Federal Penitentiary Service (FSIN) to convert the suspended sentence imposed on Alexei Navalny in a case of alleged fraud to a prison sentence. Despite the fact that his conviction in that case had been found arbitrary and unfair by the European Court of Human Rights inÂ 2018, Alexei Navalny was arrested upon his arrival at Moscow airport on 17Â January 2021.</p>\n",
    "                     </td>\n",
    "                     <td valign=\"top\" class=\"oj-table\">\n",
    "                        <p class=\"oj-tbl-txt\">2Â March 2021</p>\n",
    "                     </td>\n",
    "                  </tr>\n",
    "                  <tr class=\"oj-table\">\n",
    "                     <td valign=\"top\" class=\"oj-table\">\n",
    "                        <p class=\"oj-tbl-txt\">4.</p>\n",
    "                     </td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee7901-7524-40d9-99db-6e5e9d4199bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the identifying information regex\n",
    "\n",
    "name = cells[1].get_text(strip=True) if cells[1] else None #can keep this\n",
    "alias = cells[2].get_text(strip=True) if cells[2] else None #can keep this\n",
    "                identifying_info = cells[3].get_text(strip=False) if cells[3] else None  # Extracting the identifying information but keeping the newline \n",
    "                reasons = ' '.join(p.get_text(strip=True) for p in cells[3].find_all('p')) if cells[3] else None\n",
    "                date_of_listing = cells[4].get_text(strip=True) if cells[4] else None\n",
    "                \n",
    "identifying_information_pattern = r'>(.*?Date of Birth:.*?)</p>'\n",
    "\n",
    "\n",
    "# write patterns to extract the inform ation from the identifying information column\n",
    "position_pattern ='<p class=\"oj-tbl-txt\">(Position(s):(.*?))</p>'\n",
    "'Born on'#possible match\n",
    "dob_pattern='<p class=\"oj-tbl-txt\">DOB|Date of Birth: {1,2}/.{1,2}./{1,4}</p>'\n",
    "pob_pattern='<p class=\"oj-tbl-txt\">POB|Place of Birth:(.*?)</p>'\n",
    "nationality_pattern='<p class=\"oj-tbl-txt\">Nationality:(.*?)</p>'\n",
    "gender_pattern='<p class=\"oj-tbl-txt\">Gender:(.*?)</p>'\n",
    "passport_pattern='<p class=\"oj-tbl-txt\">Passport Number:(.*?)</p>'\n",
    "'Social media:\n",
    "Email: \n",
    "Telephone\n",
    "Address:\n",
    "Code:\n",
    "    #add a match for everything that is not matched in the above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4458c05-d388-4ff1-920e-da909dfba8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#court decision pattern needs to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e1321-37ec-4cec-940a-d2a52639f2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd1a5265-51a8-4e55-91d4-e51cc78565f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1 with 500 files.\n",
      "Processed batch 2 with 500 files.\n",
      "Processed batch 3 with 500 files.\n",
      "Processed batch 4 with 500 files.\n",
      "Processed batch 5 with 498 files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#paterns for the general listing documents\n",
    "def extract_document_info(soup):\n",
    "    # Document base information\n",
    "    url_pattern = r'href=\"(.*?\\.eu/legal-content/EN/TXT/HTML/.*?)\"'\n",
    "    celex_pattern = r'uri=CELEX%([^\"]+)'\n",
    "    \n",
    "    # Extract publication date and title\n",
    "    publication_date = soup.find('p', class_='oj-hd-date')\n",
    "    publication_date = publication_date.get_text(strip=True) if publication_date else None\n",
    "    \n",
    "    title = soup.find('p', class_='oj-doc-ti')\n",
    "    title = title.get_text(strip=True) if title else None\n",
    "    \n",
    "    # Extract URL and CELEX\n",
    "    url_match = re.search(url_pattern, str(soup))\n",
    "    celex_match = re.search(celex_pattern, str(soup))\n",
    "    url = url_match.group(1) if url_match else None\n",
    "    celex = celex_match.group(1) if celex_match else None\n",
    "    \n",
    "    # Document information patterns\n",
    "    decision_match_pattern = r'(Decision[^,]*|Regulation[^,]*|Annex[^,]*)'\n",
    "    legal_action_pattern = r'<p[^>]*>(.*?(implementing|amended|implemented).*?)</p>'\n",
    "    legal_base_pattern = r'<p[^>]*>(.*?regard to|amending.*?)</p>'\n",
    "    case_number_pattern = r'Case\\sT-\\d+/\\d+'\n",
    "    law_number_pattern = r'<p class=\"oj-hd-oj\">C (\\d+/\\d+)</p>'\n",
    "    legal_base_alt_pattern = r'\\((CFSP|EU)\\)\\s*(\\d+/\\d+)'\n",
    "    \n",
    "    decision_match = re.search(decision_match_pattern, str(soup))\n",
    "    legal_decision = re.search(legal_decision_pattern, str(soup))\n",
    "    legal_base = re.search(legal_base_pattern, str(soup))\n",
    "    case_number = re.search(case_number_pattern, str(soup))\n",
    "    law_number = re.search(law_number_pattern, str(soup))\n",
    "    legal_base_alt = re.search(legal_base_alt_pattern, str(soup))\n",
    "    \n",
    "    # List updates, removals, deletions\n",
    "    action_intended_pattern = r'<p class=\"(oj-normal|oj-bold|normal|bold)\">\\s*(.*?\\s*(?:removed|updated|deleted|replaced|amended)\\s*.*?)\\s*</p>'\n",
    "    action_intended = re.search(action_intended_pattern, str(soup))\n",
    "    \n",
    "    # Annex with numbers\n",
    "    listing_position_pattern = r'class=\"oj-normal|normal\">(\\d+.*?)'\n",
    "    name_2_pattern = r'<p class=\"oj-normal|normal\">\\s*<span class=\"oj-bold|bold\">(.*?)</span>'\n",
    "    un_sanction_date_pattern = r'class=\"oj-normal|normal\">Date of UN designation(.*?)</p>'\n",
    "    identifying_information_pattern = r'>(.*?Date of Birth:.*?)</p>'\n",
    "    \n",
    "    listing_position = re.search(listing_position_pattern, str(soup))\n",
    "    name_2 = re.search(name_2_pattern, str(soup))\n",
    "    un_sanction_date = re.search(un_sanction_date_pattern, str(soup))\n",
    "    identifying_information = re.search(identifying_information_pattern, str(soup)\n",
    "    \n",
    "    # Court decision matching\n",
    "    #name_pattern_court = r'Orders (.+?) to pay'\n",
    "    #delist_pattern = r'Annuls'\n",
    "    #delisting_case_pattern = r'Annuls (.+?);2.Orders'\n",
    "    #name_pattern2 = r'in so far as they concern (.+?);2.Orders'\n",
    "    #name_pattern1 = r'Orders (.+?) to pay'\n",
    "\n",
    "    #name_court = re.search(name_pattern_court, str(soup))\n",
    "    #delist_match = re.search(delist_pattern, str(soup))\n",
    "    #delisting_case = re.search(delisting_case_pattern, str(soup))\n",
    "    #name_case2 = re.search(name_pattern2, str(soup))\n",
    "    #name_case1 = re.search(name_pattern1, str(soup))\n",
    "    \n",
    "    # Return all extracted information\n",
    "    return {\n",
    "        'publication_date': publication_date,\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'celex': celex,\n",
    "        'decision_match': decision_match.group(1) if decision_match else None,\n",
    "        'legal_decision': legal_decision.group(1) if legal_decision else None,\n",
    "        'legal_base': legal_base.group(1) if legal_base else None,\n",
    "        'case_number': case_number.group(0) if case_number else None,\n",
    "        'law_number': law_number.group(1) if law_number else None,\n",
    "        'legal_base_alt': legal_base_alt.group(0) if legal_base_alt else None,\n",
    "        'action_intended': action_intended.group(2) if action_intended else None,\n",
    "        'listing_position': listing_position.group(1) if listing_position else None,\n",
    "        'name_2': name_2.group(1) if name_2 else None,\n",
    "        'un_sanction_date': un_sanction_date.group(1) if un_sanction_date else None,\n",
    "        'identifying_information': identifying_information.group(1) if identifying_information else None,\n",
    "        #'name_court': name_court.group(1) if name_court else None,\n",
    "        #'delist_match': delist_match.group(0) if delist_match else None,\n",
    "        #'delisting_case': delisting_case.group(1) if delisting_case else None,\n",
    "        #'name_case2': name_case2.group(1) if name_case2 else None,\n",
    "        #'name_case1': name_case1.group(1) if name_case1 else None,\n",
    "    }\n",
    "print('successfully memorized the listing patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5feaa70-8fb5-47b7-a7d2-cae365006ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process the file in batches\n",
    "def process_files_in_batches(directory, batch_size=500):\n",
    "    filenames = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    total_files = len(filenames)\n",
    "    \n",
    "    sanction_list = []\n",
    "    \n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch_filenames = filenames[i:i+batch_size]\n",
    "        \n",
    "        for filename in batch_filenames:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(file_path, encoding='utf-8') as f:\n",
    "                    data = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(file_path, encoding='latin-1') as f:\n",
    "                    data = f.read()\n",
    "                    \n",
    "            soup = BeautifulSoup(data, 'html.parser')                         \n",
    "            document_info = extract_document_info(soup)  # Extract document-level information\n",
    "            \n",
    "            if re.findall(court_decision_pattern, str(soup)): #matches all documents that are court decisions\n",
    "                court_info = extract_court_decision(soup)\n",
    "                court_decision_list =[]\n",
    "                court_decision_list.append(court_info)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            if re.findall(notice_pattern, str(soup)): #matches all documents that are notices\n",
    "                                        notice_info = notice_pattern_extraction(soup)\n",
    "                                        notice_list = []\n",
    "                                        notice = {**notice_info}\n",
    "                                        notice_list.append(notice)\n",
    "            else:\n",
    "                                        continue\n",
    "\n",
    "                                        \n",
    "            rows = soup.find_all('tr', class_='oj-table')  # Finds all elements tr = tablerow of the class oj-table\n",
    "            for row in rows[1:]:  # Skip the header row\n",
    "                cells = row.find_all('td')  # Finds all td table data in one tablerow\n",
    "                if len(cells) < 6:\n",
    "                    continue\n",
    "                name = cells[1].get_text(strip=True) if cells[1] else None\n",
    "                alias = cells[2].get_text(strip=True) if cells[2] else None\n",
    "                identifying_info = cells[3].get_text(strip=False) if cells[3] else None  # Extracting the identifying information but keeping the newline \n",
    "                reasons = ' '.join(p.get_text(strip=True) for p in cells[3].find_all('p')) if cells[3] else None\n",
    "                date_of_listing = cells[4].get_text(strip=True) if cells[4] else None\n",
    "                \n",
    "                # Combine document_info with row-specific information\n",
    "                sanction_dict = {\n",
    "                    **document_info,\n",
    "                    'Name': name,\n",
    "                    'Alias': alias,\n",
    "                    'Identifying Information': identifying_info,\n",
    "                    'Reason': reasons,\n",
    "                    'Date of Listing': date_of_listing\n",
    "                }\n",
    "                sanction_list.append(sanction_dict)\n",
    "        \n",
    "        # Optionally, save or process the current batch\n",
    "        print(f\"Processed batch {i//batch_size + 1} with {len(batch_filenames)} files.\")\n",
    "\n",
    "        # Clear sanction_list if saving batches to disk or if memory usage is a concern\n",
    "        # sanction_list.clear()\n",
    "\n",
    "    return sanction_list\n",
    "    return notice_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bed70-8eec-4216-8862-0209f6ac4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Process files in batches\n",
    "sanctions = process_files_in_batches(directory, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa83ab-5315-4b72-b8db-56ad119e25e0",
   "metadata": {},
   "source": [
    "# pickling the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4a1f429-ae81-4618-b201-0e0af46a3b26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sanctions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched_sanction_EU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:   \u001b[38;5;66;03m#Pickling\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(sanctions, fp)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sanctions' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"batched_sanction_EU\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(sanction_list, fp)\n",
    "\n",
    "with open(\"batched_notice_EU\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(notice_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4ef7bf9-0535-4cb6-9d48-a0b855a85fe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched_sanction_EU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:   \u001b[38;5;66;03m#Pickling\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mload(fp)\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "\n",
    "#with open(\"batched_sanction_EU\", \"rb\") as fp:   #Pickling\n",
    "#    pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
